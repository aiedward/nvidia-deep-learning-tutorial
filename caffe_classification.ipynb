{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Getting started with Caffe\n",
    "\n",
    "This class was created by Allison Gray and Jon Barker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!  If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"timer\" src=\"timer/timer.html\" width=\"100%\" height=\"120px\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be three: 3\n"
     ]
    }
   ],
   "source": [
    "print \"The answer should be three: \" + str(1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 15 19:16:14 2016       \r\n",
      "+------------------------------------------------------+                       \r\n",
      "| NVIDIA-SMI 346.46     Driver Version: 346.46         |                       \r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GRID K520           On   | 0000:00:03.0     Off |                  N/A |\r\n",
      "| N/A   30C    P8    18W / 125W |     10MiB /  4095MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Introduction\n",
    "\n",
    "[Caffe](http://caffe.berkeleyvision.org/) is a deep learning framework developed by the Berkely Vision and Learning Center (BVLC) and community contributors.  Caffe is released under the BSD 2-Clause license.  Caffe emphasizes easy application of deep learning. All neural networks and optimization parameters are defined by configuration files without any hard-coding and Caffe offers a command line interface as well as scripting interfaces in Python and MATLAB.  Caffe is fast due it's C++ and CUDA foundation, but the code is extensible fostering active development.  There is a large open-source community contributing many significant changes and state-of-the-art features back into Caffe.  Neural networks trained using Caffe are also saved into a well-defined binary format that makes them easy to share - in fact, there is a model zoo hosted [here](https://github.com/BVLC/caffe/wiki/Model-Zoo) where you can download cutting edge pre-trained neural networks.\n",
    "\n",
    "The objectives of this class are to learn how to complete the following tasks in Caffe:\n",
    "\n",
    "1. Build and train a convolutional neural network (CNN) for classifying images.\n",
    "2. Evaluate the classification performance of a trained CNN under different training parameter configurations.\n",
    "3. Modify the network configuration to improve classification performance.\n",
    "3. Visualize the features that a trained network has learned.\n",
    "4. Classify new test images using a trained network.\n",
    "\n",
    "This is an introductory class and is part of NVIDIA's five class Introduction to Deep Learning course.  It is assumed that you have completed the previous modules \"Introduction to Deep Learning\" and \"Getting Started with DIGITS interactive training system for image classification\" before starting this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training and Classifying with Caffe\n",
    "\n",
    "You are provided with a subset of the [ImageNet](http://www.image-net.org/) dataset.  The images are from two different categories, cats and dogs.  Below are sample images from both categories.  The cats category includes domestic cats as well as large breeds like lions and tigers.  The dog category is comprised of domestic dogs including pugs, basenji and great pyrenees.  There are approximately 13,000 images in total.\n",
    "\n",
    "![](files/src/cat1.png)\n",
    "![](files/src/cat2.png)\n",
    "![](files/src/dog1.png)\n",
    "![](files/src/dog2.png)\n",
    "\n",
    "In this class we will be creating and training a deep neural network in Caffe that can accurately classify images from these two categories, i.e. can label an image of a dog as \"dog\" and an image of a cat as \"cat\".\n",
    "\n",
    "##Task 1 - Creating a Database and Mean Image\n",
    "\n",
    "Before we train a neural network using Caffe we will move the training and validation images into a database.  The database allows Caffe to efficiently iterate over the image data during training.  The training and validation datasets are independent subsets of the original image dataset.  We will train the network using the training dataset and then test the networks performance using the validation dataset; that way we can be sure that the network performs well for images that it has never been trained on.\n",
    "\n",
    "To minimize the training time during this class we have resized all of the images to 32x32 pixels for you.  The image files can be located anywhere on the filesystem.  Caffe knows which images belong to the training and validation sets and which class each image belongs to by referring to text files `train.txt` and `val.txt`.  These files simply list the relative filename of each image tab seperated from a natural number representing the class the image belongs to.  For example, `train.txt` contains the following rows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cat/cat_0_32.jpg 0\n",
    "cat/cat_1000_32.jpg 0\n",
    "dog/dog_9990_32.jpg 1\n",
    "dog/dog_9991_32.jpg 1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a mean image from the training data. This is the image obtained by taking the mean value of each pixel across all of the training dataset images.  We do this so that we can extract that mean image from each training and validation image before it is fed into the neural network.  This is an important pre-processing step for achieving fast and effective training.  It has the effect of removing the average brightness (intensity) of each point in the image so that the network learns about image content rather than illumination conditions.\n",
    "\n",
    "We complete each of these tasks using command line tools that come with Caffe.  A number of useful utilities for data pre-processing, network training and network deployment can be found in the Caffe installation folder in `$CAFFE_ROOT/build/tools`\n",
    "\n",
    "Execute the cell below to create a mean image of the training data. (_Note_: you can ignore the \"Failed to initialize libdc1394\" warning messages in the output)\n",
    "\n",
    "You will know the lab is processing when you see a solid circle in the top-right of the window that looks like this: ![](jupyter_executing.png)\n",
    "Otherwise, when it is idle, you will see the following: ![](jupyter_idle.png)\n",
    "If you ever feel like a cell has run for to long, you can stop it with the stop button in the toolbar.\n",
    "For troubleshooting, please see [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0115 19:21:03.898493  3150 convert_imageset.cpp:143] Processed 1000 files.\n",
      "E0115 19:21:04.405110  3150 convert_imageset.cpp:143] Processed 2000 files.\n",
      "E0115 19:21:04.887454  3150 convert_imageset.cpp:143] Processed 3000 files.\n",
      "E0115 19:21:05.342878  3150 convert_imageset.cpp:143] Processed 4000 files.\n",
      "E0115 19:21:05.885934  3150 convert_imageset.cpp:143] Processed 5000 files.\n",
      "E0115 19:21:06.512353  3150 convert_imageset.cpp:143] Processed 6000 files.\n",
      "E0115 19:21:07.526595  3150 convert_imageset.cpp:143] Processed 7000 files.\n",
      "E0115 19:21:08.033902  3150 convert_imageset.cpp:143] Processed 8000 files.\n",
      "E0115 19:21:08.628804  3150 convert_imageset.cpp:143] Processed 9000 files.\n",
      "E0115 19:21:09.163836  3150 convert_imageset.cpp:143] Processed 10000 files.\n",
      "E0115 19:21:10.088855  3150 convert_imageset.cpp:143] Processed 11000 files.\n",
      "E0115 19:21:11.040606  3150 convert_imageset.cpp:143] Processed 12000 files.\n",
      "E0115 19:21:11.468827  3150 convert_imageset.cpp:149] Processed 12904 files.\n",
      "E0115 19:21:12.705242  3162 convert_imageset.cpp:143] Processed 1000 files.\n",
      "E0115 19:21:12.862079  3162 convert_imageset.cpp:149] Processed 1138 files.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf train_lmdb val_lmdb\n",
    "\n",
    "#Setup environment variables\n",
    "TOOLS=/home/ubuntu/caffe/build/tools\n",
    "TRAIN_DATA_ROOT=/home/ubuntu/data/dog_cat/dog_cat_32/train/\n",
    "VAL_DATA_ROOT=/home/ubuntu/data/dog_cat/dog_cat_32/val/\n",
    "\n",
    "#Create the training database\n",
    "$TOOLS/convert_imageset \\\n",
    "--shuffle \\\n",
    "$TRAIN_DATA_ROOT \\\n",
    "$TRAIN_DATA_ROOT/train.txt \\\n",
    "train_lmdb\n",
    "\n",
    "#Create the validation database\n",
    "$TOOLS/convert_imageset \\\n",
    "--shuffle \\\n",
    "$VAL_DATA_ROOT \\\n",
    "$VAL_DATA_ROOT/val.txt \\\n",
    "val_lmdb\n",
    "\n",
    "#Create the mean image database\n",
    "$TOOLS/compute_image_mean train_lmdb mean.binaryproto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see what the mean image looks like. Strangely, it looks a little like a mouse..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD9CAYAAACY9xrCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18a3fc4e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import caffe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "data = open('/home/ubuntu/notebook/mean.binaryproto','rb').read()\n",
    "blob.ParseFromString(data)\n",
    "arr = np.array(caffe.io.blobproto_to_array(blob))[0,:,:,:].mean(0)\n",
    "plt.imshow(arr, cmap=cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 2 - Training your network\n",
    "\n",
    "We are now going to configure our network and start training.  For this exercise we will borrow our network architecture from the cifar10 design that is provided with Caffe.  You can find the original network in `$CAFFE_ROOT/examples/cifar10`. In the first class in this series \"Introduction to Deep Learning on GPUs\" we used this network to classify the CIFAR-10 images themselves.  The only modification we will make initially is to change the final output softmax layer, which does the actual classification, to only have two classes for our dogs and cats problem rather than the ten classes needed for the CIFAR data.\n",
    "\n",
    "Below we see an image of the cifar10 network architecture.  Hopefully you will be familiar with this type of network diagram from your introduction to DIGITS in the last class.  As we can see the cifar10 network is a convolutional neural network (CNN) with three convolutional layers (each with pooling, ReLU activation and normalization) followed by a single fully-connected layer performing the final classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/src/cifar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network input data has dimensions 100x3x32x32. This means that we have batches of 100 training images each with three-channels, i.e. it's color, and 32x32 pixels.\n",
    "\n",
    "The first convolutional layer applies 32 5x5 filters.  As the filters have size 5x5 we must pad the edges of the input input images with zeros to ensure that the output of the layer has the same size.\n",
    "\n",
    "After the convolutional layer max pooling is applied to reduce the size of the output images by half, a rectified linear unit (ReLU) activation function is applied and local response normalization (LRN) is applied.\n",
    "\n",
    "Layers 2 and 3 are convolutional layers repeating this pattern but gradually reducing the output size of the layers and using average pooling.  \n",
    "\n",
    "The layers is a fully connected layer two neurons which together with the image labels feeds into a softmax layer which performs the classification and computes the classification loss.  The two final neurons in the output layer correspond to our two classes, dogs and cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caffe encodes deep neural network architectures like this in text files called prototxt files.  You can see the corresponding cifar10 prototxt file, called `train_val.prototxt`, below.  As you will see, the file is human readable and it is easy to map the sections of the prototxt file to the network layers in the diagram above.  You can get more information about the types of layers that can be defined in a network prototxt file [here](http://caffe.berkeleyvision.org/tutorial/layers.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task2\" src=\"/task2\" width=\"100%\" height=\"500px\">\n",
    " <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our datasets and a network configuration we are nearly ready to train the network.  The final thing that Caffe needs before we can train is a specification of the learning algorithm parameters.  This specification is also made in a prototxt file but with a simpler structure.  If you open the solver.prototxt file in the text editor above, you'll see it clearly specifying the learning parameters.  You can get more information about the range of parameters that can be set in a solver prototxt file [here](http://caffe.berkeleyvision.org/tutorial/solver.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are now ready to train the network.  Again, this is carried out using a  command line tool that comes with Caffe, this time it is the binary `caffe` itself with the `train` option. Execute the cell below to begin training - it should take just under a minute to train - be sure to scroll down through the complete Caffe output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0115 19:24:44.894939  3175 caffe.cpp:99] Use GPU with device ID 0\n",
      "I0115 19:24:45.037897  3175 caffe.cpp:107] Starting Optimization\n",
      "I0115 19:24:45.038094  3175 solver.cpp:32] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 250\n",
      "base_lr: 0.01\n",
      "display: 20\n",
      "max_iter: 750\n",
      "lr_policy: \"step\"\n",
      "gamma: 0.1\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "stepsize: 500\n",
      "snapshot: 250\n",
      "snapshot_prefix: \"checkpoints/snapshot\"\n",
      "solver_mode: GPU\n",
      "net: \"src/train_val.prototxt\"\n",
      "solver_type: SGD\n",
      "I0115 19:24:45.038203  3175 solver.cpp:70] Creating training net from net file: src/train_val.prototxt\n",
      "I0115 19:24:45.039489  3175 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\n",
      "I0115 19:24:45.039520  3175 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0115 19:24:45.039641  3175 net.cpp:42] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    mirror: true\n",
      "    crop_size: 32\n",
      "    mean_file: \"mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"train_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 64\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0115 19:24:45.039746  3175 layer_factory.hpp:74] Creating layer data\n",
      "I0115 19:24:45.041031  3175 net.cpp:76] Creating Layer data\n",
      "I0115 19:24:45.041054  3175 net.cpp:334] data -> data\n",
      "I0115 19:24:45.041085  3175 net.cpp:334] data -> label\n",
      "I0115 19:24:45.041105  3175 net.cpp:105] Setting up data\n",
      "I0115 19:24:45.041206  3175 db.cpp:34] Opened lmdb train_lmdb\n",
      "I0115 19:24:45.041260  3175 data_layer.cpp:67] output data size: 100,3,32,32\n",
      "I0115 19:24:45.041277  3175 data_transformer.cpp:22] Loading mean file from: mean.binaryproto\n",
      "I0115 19:24:45.042251  3175 net.cpp:112] Top shape: 100 3 32 32 (307200)\n",
      "I0115 19:24:45.042284  3175 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 19:24:45.042291  3175 layer_factory.hpp:74] Creating layer conv1\n",
      "I0115 19:24:45.042356  3175 net.cpp:76] Creating Layer conv1\n",
      "I0115 19:24:45.042369  3175 net.cpp:372] conv1 <- data\n",
      "I0115 19:24:45.042387  3175 net.cpp:334] conv1 -> conv1\n",
      "I0115 19:24:45.042404  3175 net.cpp:105] Setting up conv1\n",
      "I0115 19:24:50.000656  3175 net.cpp:112] Top shape: 100 32 32 32 (3276800)\n",
      "I0115 19:24:50.000725  3175 layer_factory.hpp:74] Creating layer pool1\n",
      "I0115 19:24:50.000746  3175 net.cpp:76] Creating Layer pool1\n",
      "I0115 19:24:50.000763  3175 net.cpp:372] pool1 <- conv1\n",
      "I0115 19:24:50.000777  3175 net.cpp:334] pool1 -> pool1\n",
      "I0115 19:24:50.000799  3175 net.cpp:105] Setting up pool1\n",
      "I0115 19:24:50.001976  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.002002  3175 layer_factory.hpp:74] Creating layer relu1\n",
      "I0115 19:24:50.002009  3175 net.cpp:76] Creating Layer relu1\n",
      "I0115 19:24:50.002023  3175 net.cpp:372] relu1 <- pool1\n",
      "I0115 19:24:50.002030  3175 net.cpp:323] relu1 -> pool1 (in-place)\n",
      "I0115 19:24:50.002045  3175 net.cpp:105] Setting up relu1\n",
      "I0115 19:24:50.002094  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.002105  3175 layer_factory.hpp:74] Creating layer norm1\n",
      "I0115 19:24:50.002116  3175 net.cpp:76] Creating Layer norm1\n",
      "I0115 19:24:50.002126  3175 net.cpp:372] norm1 <- pool1\n",
      "I0115 19:24:50.002133  3175 net.cpp:334] norm1 -> norm1\n",
      "I0115 19:24:50.002147  3175 net.cpp:105] Setting up norm1\n",
      "I0115 19:24:50.003139  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.003154  3175 layer_factory.hpp:74] Creating layer conv2\n",
      "I0115 19:24:50.003166  3175 net.cpp:76] Creating Layer conv2\n",
      "I0115 19:24:50.003170  3175 net.cpp:372] conv2 <- norm1\n",
      "I0115 19:24:50.003177  3175 net.cpp:334] conv2 -> conv2\n",
      "I0115 19:24:50.003187  3175 net.cpp:105] Setting up conv2\n",
      "I0115 19:24:50.004231  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.004254  3175 layer_factory.hpp:74] Creating layer relu2\n",
      "I0115 19:24:50.004262  3175 net.cpp:76] Creating Layer relu2\n",
      "I0115 19:24:50.004267  3175 net.cpp:372] relu2 <- conv2\n",
      "I0115 19:24:50.004273  3175 net.cpp:323] relu2 -> conv2 (in-place)\n",
      "I0115 19:24:50.004281  3175 net.cpp:105] Setting up relu2\n",
      "I0115 19:24:50.004323  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.004335  3175 layer_factory.hpp:74] Creating layer pool2\n",
      "I0115 19:24:50.004343  3175 net.cpp:76] Creating Layer pool2\n",
      "I0115 19:24:50.004349  3175 net.cpp:372] pool2 <- conv2\n",
      "I0115 19:24:50.004355  3175 net.cpp:334] pool2 -> pool2\n",
      "I0115 19:24:50.004364  3175 net.cpp:105] Setting up pool2\n",
      "I0115 19:24:50.004483  3175 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 19:24:50.004499  3175 layer_factory.hpp:74] Creating layer norm2\n",
      "I0115 19:24:50.004508  3175 net.cpp:76] Creating Layer norm2\n",
      "I0115 19:24:50.004518  3175 net.cpp:372] norm2 <- pool2\n",
      "I0115 19:24:50.004525  3175 net.cpp:334] norm2 -> norm2\n",
      "I0115 19:24:50.004534  3175 net.cpp:105] Setting up norm2\n",
      "I0115 19:24:50.004546  3175 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 19:24:50.004556  3175 layer_factory.hpp:74] Creating layer conv3\n",
      "I0115 19:24:50.004565  3175 net.cpp:76] Creating Layer conv3\n",
      "I0115 19:24:50.004570  3175 net.cpp:372] conv3 <- norm2\n",
      "I0115 19:24:50.004576  3175 net.cpp:334] conv3 -> conv3\n",
      "I0115 19:24:50.004585  3175 net.cpp:105] Setting up conv3\n",
      "I0115 19:24:50.006424  3175 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 19:24:50.006448  3175 layer_factory.hpp:74] Creating layer relu3\n",
      "I0115 19:24:50.006455  3175 net.cpp:76] Creating Layer relu3\n",
      "I0115 19:24:50.006460  3175 net.cpp:372] relu3 <- conv3\n",
      "I0115 19:24:50.006465  3175 net.cpp:323] relu3 -> conv3 (in-place)\n",
      "I0115 19:24:50.006474  3175 net.cpp:105] Setting up relu3\n",
      "I0115 19:24:50.006516  3175 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 19:24:50.006527  3175 layer_factory.hpp:74] Creating layer pool3\n",
      "I0115 19:24:50.006536  3175 net.cpp:76] Creating Layer pool3\n",
      "I0115 19:24:50.006539  3175 net.cpp:372] pool3 <- conv3\n",
      "I0115 19:24:50.006587  3175 net.cpp:334] pool3 -> pool3\n",
      "I0115 19:24:50.006594  3175 net.cpp:105] Setting up pool3\n",
      "I0115 19:24:50.006639  3175 net.cpp:112] Top shape: 100 64 4 4 (102400)\n",
      "I0115 19:24:50.006655  3175 layer_factory.hpp:74] Creating layer ip1\n",
      "I0115 19:24:50.006680  3175 net.cpp:76] Creating Layer ip1\n",
      "I0115 19:24:50.006711  3175 net.cpp:372] ip1 <- pool3\n",
      "I0115 19:24:50.006741  3175 net.cpp:334] ip1 -> ip1\n",
      "I0115 19:24:50.006757  3175 net.cpp:105] Setting up ip1\n",
      "I0115 19:24:50.006834  3175 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 19:24:50.006847  3175 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 19:24:50.006857  3175 net.cpp:76] Creating Layer loss\n",
      "I0115 19:24:50.006862  3175 net.cpp:372] loss <- ip1\n",
      "I0115 19:24:50.006871  3175 net.cpp:372] loss <- label\n",
      "I0115 19:24:50.006878  3175 net.cpp:334] loss -> loss\n",
      "I0115 19:24:50.006886  3175 net.cpp:105] Setting up loss\n",
      "I0115 19:24:50.006901  3175 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 19:24:50.006971  3175 net.cpp:112] Top shape: 1 1 1 1 (1)\n",
      "I0115 19:24:50.006983  3175 net.cpp:118]     with loss weight 1\n",
      "I0115 19:24:50.007014  3175 net.cpp:163] loss needs backward computation.\n",
      "I0115 19:24:50.007019  3175 net.cpp:163] ip1 needs backward computation.\n",
      "I0115 19:24:50.007024  3175 net.cpp:163] pool3 needs backward computation.\n",
      "I0115 19:24:50.007027  3175 net.cpp:163] relu3 needs backward computation.\n",
      "I0115 19:24:50.007032  3175 net.cpp:163] conv3 needs backward computation.\n",
      "I0115 19:24:50.007036  3175 net.cpp:163] norm2 needs backward computation.\n",
      "I0115 19:24:50.007040  3175 net.cpp:163] pool2 needs backward computation.\n",
      "I0115 19:24:50.007045  3175 net.cpp:163] relu2 needs backward computation.\n",
      "I0115 19:24:50.007050  3175 net.cpp:163] conv2 needs backward computation.\n",
      "I0115 19:24:50.007055  3175 net.cpp:163] norm1 needs backward computation.\n",
      "I0115 19:24:50.007058  3175 net.cpp:163] relu1 needs backward computation.\n",
      "I0115 19:24:50.007063  3175 net.cpp:163] pool1 needs backward computation.\n",
      "I0115 19:24:50.007067  3175 net.cpp:163] conv1 needs backward computation.\n",
      "I0115 19:24:50.007071  3175 net.cpp:165] data does not need backward computation.\n",
      "I0115 19:24:50.007076  3175 net.cpp:201] This network produces output loss\n",
      "I0115 19:24:50.007087  3175 net.cpp:446] Collecting Learning Rate and Weight Decay.\n",
      "I0115 19:24:50.007098  3175 net.cpp:213] Network initialization done.\n",
      "I0115 19:24:50.007103  3175 net.cpp:214] Memory required for data: 36046004\n",
      "I0115 19:24:50.007513  3175 solver.cpp:154] Creating test net (#0) specified by net file: src/train_val.prototxt\n",
      "I0115 19:24:50.007587  3175 net.cpp:253] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0115 19:24:50.007742  3175 net.cpp:42] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    crop_size: 32\n",
      "    mean_file: \"mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"val_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 64\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "I0115 19:24:50.007853  3175 layer_factory.hpp:74] Creating layer data\n",
      "I0115 19:24:50.007870  3175 net.cpp:76] Creating Layer data\n",
      "I0115 19:24:50.007876  3175 net.cpp:334] data -> data\n",
      "I0115 19:24:50.007891  3175 net.cpp:334] data -> label\n",
      "I0115 19:24:50.007899  3175 net.cpp:105] Setting up data\n",
      "I0115 19:24:50.007953  3175 db.cpp:34] Opened lmdb val_lmdb\n",
      "I0115 19:24:50.007982  3175 data_layer.cpp:67] output data size: 100,3,32,32\n",
      "I0115 19:24:50.007994  3175 data_transformer.cpp:22] Loading mean file from: mean.binaryproto\n",
      "I0115 19:24:50.008157  3175 net.cpp:112] Top shape: 100 3 32 32 (307200)\n",
      "I0115 19:24:50.008172  3175 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 19:24:50.008177  3175 layer_factory.hpp:74] Creating layer label_data_1_split\n",
      "I0115 19:24:50.008188  3175 net.cpp:76] Creating Layer label_data_1_split\n",
      "I0115 19:24:50.008191  3175 net.cpp:372] label_data_1_split <- label\n",
      "I0115 19:24:50.008198  3175 net.cpp:334] label_data_1_split -> label_data_1_split_0\n",
      "I0115 19:24:50.008208  3175 net.cpp:334] label_data_1_split -> label_data_1_split_1\n",
      "I0115 19:24:50.008214  3175 net.cpp:105] Setting up label_data_1_split\n",
      "I0115 19:24:50.008221  3175 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 19:24:50.008225  3175 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 19:24:50.008230  3175 layer_factory.hpp:74] Creating layer conv1\n",
      "I0115 19:24:50.008258  3175 net.cpp:76] Creating Layer conv1\n",
      "I0115 19:24:50.008263  3175 net.cpp:372] conv1 <- data\n",
      "I0115 19:24:50.008271  3175 net.cpp:334] conv1 -> conv1\n",
      "I0115 19:24:50.008281  3175 net.cpp:105] Setting up conv1\n",
      "I0115 19:24:50.008597  3175 net.cpp:112] Top shape: 100 32 32 32 (3276800)\n",
      "I0115 19:24:50.008620  3175 layer_factory.hpp:74] Creating layer pool1\n",
      "I0115 19:24:50.008630  3175 net.cpp:76] Creating Layer pool1\n",
      "I0115 19:24:50.008633  3175 net.cpp:372] pool1 <- conv1\n",
      "I0115 19:24:50.008640  3175 net.cpp:334] pool1 -> pool1\n",
      "I0115 19:24:50.008647  3175 net.cpp:105] Setting up pool1\n",
      "I0115 19:24:50.008774  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.008796  3175 layer_factory.hpp:74] Creating layer relu1\n",
      "I0115 19:24:50.008806  3175 net.cpp:76] Creating Layer relu1\n",
      "I0115 19:24:50.008811  3175 net.cpp:372] relu1 <- pool1\n",
      "I0115 19:24:50.008816  3175 net.cpp:323] relu1 -> pool1 (in-place)\n",
      "I0115 19:24:50.008822  3175 net.cpp:105] Setting up relu1\n",
      "I0115 19:24:50.008867  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.008877  3175 layer_factory.hpp:74] Creating layer norm1\n",
      "I0115 19:24:50.008884  3175 net.cpp:76] Creating Layer norm1\n",
      "I0115 19:24:50.008888  3175 net.cpp:372] norm1 <- pool1\n",
      "I0115 19:24:50.008894  3175 net.cpp:334] norm1 -> norm1\n",
      "I0115 19:24:50.008919  3175 net.cpp:105] Setting up norm1\n",
      "I0115 19:24:50.008934  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.008944  3175 layer_factory.hpp:74] Creating layer conv2\n",
      "I0115 19:24:50.008950  3175 net.cpp:76] Creating Layer conv2\n",
      "I0115 19:24:50.008955  3175 net.cpp:372] conv2 <- norm1\n",
      "I0115 19:24:50.008961  3175 net.cpp:334] conv2 -> conv2\n",
      "I0115 19:24:50.008967  3175 net.cpp:105] Setting up conv2\n",
      "I0115 19:24:50.010045  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.010064  3175 layer_factory.hpp:74] Creating layer relu2\n",
      "I0115 19:24:50.010072  3175 net.cpp:76] Creating Layer relu2\n",
      "I0115 19:24:50.010076  3175 net.cpp:372] relu2 <- conv2\n",
      "I0115 19:24:50.010082  3175 net.cpp:323] relu2 -> conv2 (in-place)\n",
      "I0115 19:24:50.010087  3175 net.cpp:105] Setting up relu2\n",
      "I0115 19:24:50.010131  3175 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 19:24:50.010143  3175 layer_factory.hpp:74] Creating layer pool2\n",
      "I0115 19:24:50.010150  3175 net.cpp:76] Creating Layer pool2\n",
      "I0115 19:24:50.010154  3175 net.cpp:372] pool2 <- conv2\n",
      "I0115 19:24:50.010162  3175 net.cpp:334] pool2 -> pool2\n",
      "I0115 19:24:50.010169  3175 net.cpp:105] Setting up pool2\n",
      "I0115 19:24:50.010215  3175 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 19:24:50.010227  3175 layer_factory.hpp:74] Creating layer norm2\n",
      "I0115 19:24:50.010233  3175 net.cpp:76] Creating Layer norm2\n",
      "I0115 19:24:50.010237  3175 net.cpp:372] norm2 <- pool2\n",
      "I0115 19:24:50.010244  3175 net.cpp:334] norm2 -> norm2\n",
      "I0115 19:24:50.010251  3175 net.cpp:105] Setting up norm2\n",
      "I0115 19:24:50.010267  3175 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 19:24:50.010274  3175 layer_factory.hpp:74] Creating layer conv3\n",
      "I0115 19:24:50.010280  3175 net.cpp:76] Creating Layer conv3\n",
      "I0115 19:24:50.010289  3175 net.cpp:372] conv3 <- norm2\n",
      "I0115 19:24:50.010295  3175 net.cpp:334] conv3 -> conv3\n",
      "I0115 19:24:50.010301  3175 net.cpp:105] Setting up conv3\n",
      "I0115 19:24:50.012189  3175 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 19:24:50.012214  3175 layer_factory.hpp:74] Creating layer relu3\n",
      "I0115 19:24:50.012223  3175 net.cpp:76] Creating Layer relu3\n",
      "I0115 19:24:50.012228  3175 net.cpp:372] relu3 <- conv3\n",
      "I0115 19:24:50.012234  3175 net.cpp:323] relu3 -> conv3 (in-place)\n",
      "I0115 19:24:50.012245  3175 net.cpp:105] Setting up relu3\n",
      "I0115 19:24:50.012365  3175 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 19:24:50.012382  3175 layer_factory.hpp:74] Creating layer pool3\n",
      "I0115 19:24:50.012388  3175 net.cpp:76] Creating Layer pool3\n",
      "I0115 19:24:50.012394  3175 net.cpp:372] pool3 <- conv3\n",
      "I0115 19:24:50.012400  3175 net.cpp:334] pool3 -> pool3\n",
      "I0115 19:24:50.012413  3175 net.cpp:105] Setting up pool3\n",
      "I0115 19:24:50.012455  3175 net.cpp:112] Top shape: 100 64 4 4 (102400)\n",
      "I0115 19:24:50.012466  3175 layer_factory.hpp:74] Creating layer ip1\n",
      "I0115 19:24:50.012475  3175 net.cpp:76] Creating Layer ip1\n",
      "I0115 19:24:50.012480  3175 net.cpp:372] ip1 <- pool3\n",
      "I0115 19:24:50.012486  3175 net.cpp:334] ip1 -> ip1\n",
      "I0115 19:24:50.012495  3175 net.cpp:105] Setting up ip1\n",
      "I0115 19:24:50.012580  3175 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 19:24:50.012593  3175 layer_factory.hpp:74] Creating layer ip1_ip1_0_split\n",
      "I0115 19:24:50.012599  3175 net.cpp:76] Creating Layer ip1_ip1_0_split\n",
      "I0115 19:24:50.012609  3175 net.cpp:372] ip1_ip1_0_split <- ip1\n",
      "I0115 19:24:50.012615  3175 net.cpp:334] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0115 19:24:50.012624  3175 net.cpp:334] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0115 19:24:50.012629  3175 net.cpp:105] Setting up ip1_ip1_0_split\n",
      "I0115 19:24:50.012639  3175 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 19:24:50.012644  3175 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 19:24:50.012647  3175 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 19:24:50.012653  3175 net.cpp:76] Creating Layer loss\n",
      "I0115 19:24:50.012657  3175 net.cpp:372] loss <- ip1_ip1_0_split_0\n",
      "I0115 19:24:50.012663  3175 net.cpp:372] loss <- label_data_1_split_0\n",
      "I0115 19:24:50.012670  3175 net.cpp:334] loss -> loss\n",
      "I0115 19:24:50.012679  3175 net.cpp:105] Setting up loss\n",
      "I0115 19:24:50.012704  3175 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 19:24:50.012755  3175 net.cpp:112] Top shape: 1 1 1 1 (1)\n",
      "I0115 19:24:50.012766  3175 net.cpp:118]     with loss weight 1\n",
      "I0115 19:24:50.012775  3175 layer_factory.hpp:74] Creating layer accuracy\n",
      "I0115 19:24:50.012787  3175 net.cpp:76] Creating Layer accuracy\n",
      "I0115 19:24:50.012796  3175 net.cpp:372] accuracy <- ip1_ip1_0_split_1\n",
      "I0115 19:24:50.012802  3175 net.cpp:372] accuracy <- label_data_1_split_1\n",
      "I0115 19:24:50.012809  3175 net.cpp:334] accuracy -> accuracy\n",
      "I0115 19:24:50.012817  3175 net.cpp:105] Setting up accuracy\n",
      "I0115 19:24:50.012826  3175 net.cpp:112] Top shape: 1 1 1 1 (1)\n",
      "I0115 19:24:50.012832  3175 net.cpp:165] accuracy does not need backward computation.\n",
      "I0115 19:24:50.012836  3175 net.cpp:163] loss needs backward computation.\n",
      "I0115 19:24:50.012842  3175 net.cpp:163] ip1_ip1_0_split needs backward computation.\n",
      "I0115 19:24:50.012846  3175 net.cpp:163] ip1 needs backward computation.\n",
      "I0115 19:24:50.012852  3175 net.cpp:163] pool3 needs backward computation.\n",
      "I0115 19:24:50.012856  3175 net.cpp:163] relu3 needs backward computation.\n",
      "I0115 19:24:50.012859  3175 net.cpp:163] conv3 needs backward computation.\n",
      "I0115 19:24:50.012864  3175 net.cpp:163] norm2 needs backward computation.\n",
      "I0115 19:24:50.012868  3175 net.cpp:163] pool2 needs backward computation.\n",
      "I0115 19:24:50.012874  3175 net.cpp:163] relu2 needs backward computation.\n",
      "I0115 19:24:50.012877  3175 net.cpp:163] conv2 needs backward computation.\n",
      "I0115 19:24:50.012881  3175 net.cpp:163] norm1 needs backward computation.\n",
      "I0115 19:24:50.012884  3175 net.cpp:163] relu1 needs backward computation.\n",
      "I0115 19:24:50.012889  3175 net.cpp:163] pool1 needs backward computation.\n",
      "I0115 19:24:50.012893  3175 net.cpp:163] conv1 needs backward computation.\n",
      "I0115 19:24:50.012897  3175 net.cpp:165] label_data_1_split does not need backward computation.\n",
      "I0115 19:24:50.012902  3175 net.cpp:165] data does not need backward computation.\n",
      "I0115 19:24:50.012905  3175 net.cpp:201] This network produces output accuracy\n",
      "I0115 19:24:50.012912  3175 net.cpp:201] This network produces output loss\n",
      "I0115 19:24:50.012923  3175 net.cpp:446] Collecting Learning Rate and Weight Decay.\n",
      "I0115 19:24:50.012934  3175 net.cpp:213] Network initialization done.\n",
      "I0115 19:24:50.012938  3175 net.cpp:214] Memory required for data: 36048408\n",
      "I0115 19:24:50.012981  3175 solver.cpp:42] Solver scaffolding done.\n",
      "I0115 19:24:50.013008  3175 solver.cpp:222] Solving \n",
      "I0115 19:24:50.013018  3175 solver.cpp:223] Learning Rate Policy: step\n",
      "I0115 19:24:50.013028  3175 solver.cpp:266] Iteration 0, Testing net (#0)\n",
      "I0115 19:24:50.834151  3175 solver.cpp:315]     Test net output #0: accuracy = 0.5071\n",
      "I0115 19:24:50.834210  3175 solver.cpp:315]     Test net output #1: loss = 0.693148 (* 1 = 0.693148 loss)\n",
      "I0115 19:24:50.845659  3175 solver.cpp:189] Iteration 0, loss = 0.693108\n",
      "I0115 19:24:50.845688  3175 solver.cpp:204]     Train net output #0: loss = 0.693108 (* 1 = 0.693108 loss)\n",
      "I0115 19:24:50.845710  3175 solver.cpp:470] Iteration 0, lr = 0.01\n",
      "I0115 19:24:51.297634  3175 solver.cpp:189] Iteration 20, loss = 0.691847\n",
      "I0115 19:24:51.297689  3175 solver.cpp:204]     Train net output #0: loss = 0.691847 (* 1 = 0.691847 loss)\n",
      "I0115 19:24:51.297698  3175 solver.cpp:470] Iteration 20, lr = 0.01\n",
      "I0115 19:24:51.749534  3175 solver.cpp:189] Iteration 40, loss = 0.682605\n",
      "I0115 19:24:51.749578  3175 solver.cpp:204]     Train net output #0: loss = 0.682605 (* 1 = 0.682605 loss)\n",
      "I0115 19:24:51.749588  3175 solver.cpp:470] Iteration 40, lr = 0.01\n",
      "I0115 19:24:52.201936  3175 solver.cpp:189] Iteration 60, loss = 0.671296\n",
      "I0115 19:24:52.201980  3175 solver.cpp:204]     Train net output #0: loss = 0.671296 (* 1 = 0.671296 loss)\n",
      "I0115 19:24:52.201989  3175 solver.cpp:470] Iteration 60, lr = 0.01\n",
      "I0115 19:24:52.654203  3175 solver.cpp:189] Iteration 80, loss = 0.72983\n",
      "I0115 19:24:52.654253  3175 solver.cpp:204]     Train net output #0: loss = 0.72983 (* 1 = 0.72983 loss)\n",
      "I0115 19:24:52.654261  3175 solver.cpp:470] Iteration 80, lr = 0.01\n",
      "I0115 19:24:53.106828  3175 solver.cpp:189] Iteration 100, loss = 0.692547\n",
      "I0115 19:24:53.106875  3175 solver.cpp:204]     Train net output #0: loss = 0.692547 (* 1 = 0.692547 loss)\n",
      "I0115 19:24:53.106884  3175 solver.cpp:470] Iteration 100, lr = 0.01\n",
      "I0115 19:24:53.558854  3175 solver.cpp:189] Iteration 120, loss = 0.707352\n",
      "I0115 19:24:53.558897  3175 solver.cpp:204]     Train net output #0: loss = 0.707352 (* 1 = 0.707352 loss)\n",
      "I0115 19:24:53.558905  3175 solver.cpp:470] Iteration 120, lr = 0.01\n",
      "I0115 19:24:54.011162  3175 solver.cpp:189] Iteration 140, loss = 0.648271\n",
      "I0115 19:24:54.011200  3175 solver.cpp:204]     Train net output #0: loss = 0.648271 (* 1 = 0.648271 loss)\n",
      "I0115 19:24:54.011209  3175 solver.cpp:470] Iteration 140, lr = 0.01\n",
      "I0115 19:24:54.462982  3175 solver.cpp:189] Iteration 160, loss = 0.640947\n",
      "I0115 19:24:54.463029  3175 solver.cpp:204]     Train net output #0: loss = 0.640947 (* 1 = 0.640947 loss)\n",
      "I0115 19:24:54.463038  3175 solver.cpp:470] Iteration 160, lr = 0.01\n",
      "I0115 19:24:54.914799  3175 solver.cpp:189] Iteration 180, loss = 0.658329\n",
      "I0115 19:24:54.914842  3175 solver.cpp:204]     Train net output #0: loss = 0.658329 (* 1 = 0.658329 loss)\n",
      "I0115 19:24:54.914851  3175 solver.cpp:470] Iteration 180, lr = 0.01\n",
      "I0115 19:24:55.367202  3175 solver.cpp:189] Iteration 200, loss = 0.709169\n",
      "I0115 19:24:55.367249  3175 solver.cpp:204]     Train net output #0: loss = 0.709169 (* 1 = 0.709169 loss)\n",
      "I0115 19:24:55.367257  3175 solver.cpp:470] Iteration 200, lr = 0.01\n",
      "I0115 19:24:55.819794  3175 solver.cpp:189] Iteration 220, loss = 0.636351\n",
      "I0115 19:24:55.819839  3175 solver.cpp:204]     Train net output #0: loss = 0.636351 (* 1 = 0.636351 loss)\n",
      "I0115 19:24:55.819849  3175 solver.cpp:470] Iteration 220, lr = 0.01\n",
      "I0115 19:24:56.272135  3175 solver.cpp:189] Iteration 240, loss = 0.654862\n",
      "I0115 19:24:56.272181  3175 solver.cpp:204]     Train net output #0: loss = 0.654862 (* 1 = 0.654862 loss)\n",
      "I0115 19:24:56.272189  3175 solver.cpp:470] Iteration 240, lr = 0.01\n",
      "I0115 19:24:56.490496  3175 solver.cpp:334] Snapshotting to checkpoints/snapshot_iter_250.caffemodel\n",
      "I0115 19:24:56.491928  3175 solver.cpp:342] Snapshotting solver state to checkpoints/snapshot_iter_250.solverstate\n",
      "I0115 19:24:56.492564  3175 solver.cpp:266] Iteration 250, Testing net (#0)\n",
      "I0115 19:24:57.299710  3175 solver.cpp:315]     Test net output #0: accuracy = 0.5541\n",
      "I0115 19:24:57.299756  3175 solver.cpp:315]     Test net output #1: loss = 0.685937 (* 1 = 0.685937 loss)\n",
      "I0115 19:24:57.534894  3175 solver.cpp:189] Iteration 260, loss = 0.683511\n",
      "I0115 19:24:57.534930  3175 solver.cpp:204]     Train net output #0: loss = 0.683511 (* 1 = 0.683511 loss)\n",
      "I0115 19:24:57.534940  3175 solver.cpp:470] Iteration 260, lr = 0.01\n",
      "I0115 19:24:57.987815  3175 solver.cpp:189] Iteration 280, loss = 0.665001\n",
      "I0115 19:24:57.987854  3175 solver.cpp:204]     Train net output #0: loss = 0.665001 (* 1 = 0.665001 loss)\n",
      "I0115 19:24:57.987864  3175 solver.cpp:470] Iteration 280, lr = 0.01\n",
      "I0115 19:24:58.440629  3175 solver.cpp:189] Iteration 300, loss = 0.675635\n",
      "I0115 19:24:58.440680  3175 solver.cpp:204]     Train net output #0: loss = 0.675635 (* 1 = 0.675635 loss)\n",
      "I0115 19:24:58.440688  3175 solver.cpp:470] Iteration 300, lr = 0.01\n",
      "I0115 19:24:58.894033  3175 solver.cpp:189] Iteration 320, loss = 0.676924\n",
      "I0115 19:24:58.894081  3175 solver.cpp:204]     Train net output #0: loss = 0.676924 (* 1 = 0.676924 loss)\n",
      "I0115 19:24:58.894090  3175 solver.cpp:470] Iteration 320, lr = 0.01\n",
      "I0115 19:24:59.346931  3175 solver.cpp:189] Iteration 340, loss = 0.608669\n",
      "I0115 19:24:59.346976  3175 solver.cpp:204]     Train net output #0: loss = 0.608669 (* 1 = 0.608669 loss)\n",
      "I0115 19:24:59.346985  3175 solver.cpp:470] Iteration 340, lr = 0.01\n",
      "I0115 19:24:59.799343  3175 solver.cpp:189] Iteration 360, loss = 0.613595\n",
      "I0115 19:24:59.799394  3175 solver.cpp:204]     Train net output #0: loss = 0.613595 (* 1 = 0.613595 loss)\n",
      "I0115 19:24:59.799403  3175 solver.cpp:470] Iteration 360, lr = 0.01\n",
      "I0115 19:25:00.252339  3175 solver.cpp:189] Iteration 380, loss = 0.645065\n",
      "I0115 19:25:00.252390  3175 solver.cpp:204]     Train net output #0: loss = 0.645065 (* 1 = 0.645065 loss)\n",
      "I0115 19:25:00.252435  3175 solver.cpp:470] Iteration 380, lr = 0.01\n",
      "I0115 19:25:00.704706  3175 solver.cpp:189] Iteration 400, loss = 0.603425\n",
      "I0115 19:25:00.704761  3175 solver.cpp:204]     Train net output #0: loss = 0.603425 (* 1 = 0.603425 loss)\n",
      "I0115 19:25:00.704769  3175 solver.cpp:470] Iteration 400, lr = 0.01\n",
      "I0115 19:25:01.157702  3175 solver.cpp:189] Iteration 420, loss = 0.622494\n",
      "I0115 19:25:01.157747  3175 solver.cpp:204]     Train net output #0: loss = 0.622494 (* 1 = 0.622494 loss)\n",
      "I0115 19:25:01.157755  3175 solver.cpp:470] Iteration 420, lr = 0.01\n",
      "I0115 19:25:01.612793  3175 solver.cpp:189] Iteration 440, loss = 0.661154\n",
      "I0115 19:25:01.612848  3175 solver.cpp:204]     Train net output #0: loss = 0.661154 (* 1 = 0.661154 loss)\n",
      "I0115 19:25:01.612859  3175 solver.cpp:470] Iteration 440, lr = 0.01\n",
      "I0115 19:25:02.066388  3175 solver.cpp:189] Iteration 460, loss = 0.676445\n",
      "I0115 19:25:02.066440  3175 solver.cpp:204]     Train net output #0: loss = 0.676445 (* 1 = 0.676445 loss)\n",
      "I0115 19:25:02.066449  3175 solver.cpp:470] Iteration 460, lr = 0.01\n",
      "I0115 19:25:02.518872  3175 solver.cpp:189] Iteration 480, loss = 0.594432\n",
      "I0115 19:25:02.518934  3175 solver.cpp:204]     Train net output #0: loss = 0.594432 (* 1 = 0.594432 loss)\n",
      "I0115 19:25:02.518942  3175 solver.cpp:470] Iteration 480, lr = 0.01\n",
      "I0115 19:25:02.963594  3175 solver.cpp:334] Snapshotting to checkpoints/snapshot_iter_500.caffemodel\n",
      "I0115 19:25:02.964798  3175 solver.cpp:342] Snapshotting solver state to checkpoints/snapshot_iter_500.solverstate\n",
      "I0115 19:25:02.965453  3175 solver.cpp:266] Iteration 500, Testing net (#0)\n",
      "I0115 19:25:03.773658  3175 solver.cpp:315]     Test net output #0: accuracy = 0.6678\n",
      "I0115 19:25:03.773717  3175 solver.cpp:315]     Test net output #1: loss = 0.601895 (* 1 = 0.601895 loss)\n",
      "I0115 19:25:03.782374  3175 solver.cpp:189] Iteration 500, loss = 0.622605\n",
      "I0115 19:25:03.782407  3175 solver.cpp:204]     Train net output #0: loss = 0.622605 (* 1 = 0.622605 loss)\n",
      "I0115 19:25:03.782416  3175 solver.cpp:470] Iteration 500, lr = 0.001\n",
      "I0115 19:25:04.235890  3175 solver.cpp:189] Iteration 520, loss = 0.660622\n",
      "I0115 19:25:04.235940  3175 solver.cpp:204]     Train net output #0: loss = 0.660622 (* 1 = 0.660622 loss)\n",
      "I0115 19:25:04.235949  3175 solver.cpp:470] Iteration 520, lr = 0.001\n",
      "I0115 19:25:04.688608  3175 solver.cpp:189] Iteration 540, loss = 0.596291\n",
      "I0115 19:25:04.688666  3175 solver.cpp:204]     Train net output #0: loss = 0.596291 (* 1 = 0.596291 loss)\n",
      "I0115 19:25:04.688675  3175 solver.cpp:470] Iteration 540, lr = 0.001\n",
      "I0115 19:25:05.141337  3175 solver.cpp:189] Iteration 560, loss = 0.570198\n",
      "I0115 19:25:05.141386  3175 solver.cpp:204]     Train net output #0: loss = 0.570198 (* 1 = 0.570198 loss)\n",
      "I0115 19:25:05.141396  3175 solver.cpp:470] Iteration 560, lr = 0.001\n",
      "I0115 19:25:05.593940  3175 solver.cpp:189] Iteration 580, loss = 0.555554\n",
      "I0115 19:25:05.593989  3175 solver.cpp:204]     Train net output #0: loss = 0.555554 (* 1 = 0.555554 loss)\n",
      "I0115 19:25:05.593998  3175 solver.cpp:470] Iteration 580, lr = 0.001\n",
      "I0115 19:25:06.046639  3175 solver.cpp:189] Iteration 600, loss = 0.503012\n",
      "I0115 19:25:06.046690  3175 solver.cpp:204]     Train net output #0: loss = 0.503012 (* 1 = 0.503012 loss)\n",
      "I0115 19:25:06.046700  3175 solver.cpp:470] Iteration 600, lr = 0.001\n",
      "I0115 19:25:06.499136  3175 solver.cpp:189] Iteration 620, loss = 0.568871\n",
      "I0115 19:25:06.499182  3175 solver.cpp:204]     Train net output #0: loss = 0.568871 (* 1 = 0.568871 loss)\n",
      "I0115 19:25:06.499191  3175 solver.cpp:470] Iteration 620, lr = 0.001\n",
      "I0115 19:25:06.951324  3175 solver.cpp:189] Iteration 640, loss = 0.555774\n",
      "I0115 19:25:06.951367  3175 solver.cpp:204]     Train net output #0: loss = 0.555774 (* 1 = 0.555774 loss)\n",
      "I0115 19:25:06.951376  3175 solver.cpp:470] Iteration 640, lr = 0.001\n",
      "I0115 19:25:07.403666  3175 solver.cpp:189] Iteration 660, loss = 0.583044\n",
      "I0115 19:25:07.403712  3175 solver.cpp:204]     Train net output #0: loss = 0.583044 (* 1 = 0.583044 loss)\n",
      "I0115 19:25:07.403759  3175 solver.cpp:470] Iteration 660, lr = 0.001\n",
      "I0115 19:25:07.855311  3175 solver.cpp:189] Iteration 680, loss = 0.64829\n",
      "I0115 19:25:07.855360  3175 solver.cpp:204]     Train net output #0: loss = 0.64829 (* 1 = 0.64829 loss)\n",
      "I0115 19:25:07.855368  3175 solver.cpp:470] Iteration 680, lr = 0.001\n",
      "I0115 19:25:08.306988  3175 solver.cpp:189] Iteration 700, loss = 0.580546\n",
      "I0115 19:25:08.307037  3175 solver.cpp:204]     Train net output #0: loss = 0.580546 (* 1 = 0.580546 loss)\n",
      "I0115 19:25:08.307045  3175 solver.cpp:470] Iteration 700, lr = 0.001\n",
      "I0115 19:25:08.759356  3175 solver.cpp:189] Iteration 720, loss = 0.604142\n",
      "I0115 19:25:08.759414  3175 solver.cpp:204]     Train net output #0: loss = 0.604142 (* 1 = 0.604142 loss)\n",
      "I0115 19:25:08.759423  3175 solver.cpp:470] Iteration 720, lr = 0.001\n",
      "I0115 19:25:09.212354  3175 solver.cpp:189] Iteration 740, loss = 0.50448\n",
      "I0115 19:25:09.212406  3175 solver.cpp:204]     Train net output #0: loss = 0.50448 (* 1 = 0.50448 loss)\n",
      "I0115 19:25:09.212415  3175 solver.cpp:470] Iteration 740, lr = 0.001\n",
      "I0115 19:25:09.430464  3175 solver.cpp:334] Snapshotting to checkpoints/snapshot_iter_750.caffemodel\n",
      "I0115 19:25:09.432682  3175 solver.cpp:342] Snapshotting solver state to checkpoints/snapshot_iter_750.solverstate\n",
      "I0115 19:25:09.434298  3175 solver.cpp:266] Iteration 750, Testing net (#0)\n",
      "I0115 19:25:10.240154  3175 solver.cpp:315]     Test net output #0: accuracy = 0.6959\n",
      "I0115 19:25:10.240203  3175 solver.cpp:315]     Test net output #1: loss = 0.586884 (* 1 = 0.586884 loss)\n",
      "I0115 19:25:10.240211  3175 solver.cpp:253] Optimization Done.\n",
      "I0115 19:25:10.240216  3175 caffe.cpp:121] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#Set the location of the caffe tools folder\n",
    "TOOLS=/home/ubuntu/caffe/build/tools\n",
    "#Train the network\n",
    "$TOOLS/caffe train -gpu 0 -solver src/solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Q #1: \n",
    "After 250, 500, and 750 iterations what is your training accuracy?\n",
    "\n",
    "**A**: See [Answer #1 below](#Answer-#1)\n",
    "\n",
    "It turns out that we can achieve much higher accuracy and a lower loss against both the training and validation datasets for this task. This means our network is *underfitting* the data.  Essentially, this means that our ability to approximate the function that maps raw images to dog and cat labels is constrained by the number of trainable parameters in our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 3 - Modifying your Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many network configurations that you may have heard of such as Alexnet, GoogLeNet and VGG are significantly larger than the three layer architecture used above and have proven to be very accurate at classifying the ImageNet images.  You are going to increase the complexity of this network to improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many knobs that one can turn in choosing a neural network architecture.  For example, you can add layers, increased the number of learned weights, change the learning rate or introduce a more complex policy to modify the learning rate as training progresses.  We will experiment with some of these modifications to see the effect on classificaiton accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First modify the network architecture in the `train_val2.protoxt` file below to increase the number of outputs in the convolutional layers to 64 for layer 1, 256 for layer 2 and 256 for layer 3.\n",
    "\n",
    "Also modify the learning parameters in the `solver2.prototxt` file to decrease the learning rate to 0.005, while keeping everything else the same.\n",
    "\n",
    "If you have any difficulties with making the right modifications you can find the answers in `train_val2.answer.prototxt` and `solver2.answer.prototxt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task3\" src=\"/task3\" width=\"100%\" height=\"500px\">\n",
    " <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have saved your changes we can retrain the network by executing the cell below.  This time it will take about 90 seconds to train due to the increased network size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0115 20:00:05.418829  4555 caffe.cpp:99] Use GPU with device ID 0\n",
      "I0115 20:00:05.567639  4555 caffe.cpp:107] Starting Optimization\n",
      "I0115 20:00:05.567771  4555 solver.cpp:32] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 250\n",
      "base_lr: 0.005\n",
      "display: 20\n",
      "max_iter: 750\n",
      "lr_policy: \"step\"\n",
      "gamma: 0.1\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "stepsize: 500\n",
      "snapshot: 250\n",
      "snapshot_prefix: \"checkpoints/snapshot\"\n",
      "solver_mode: GPU\n",
      "net: \"src/train_val.prototxt\"\n",
      "solver_type: SGD\n",
      "I0115 20:00:05.567811  4555 solver.cpp:70] Creating training net from net file: src/train_val.prototxt\n",
      "I0115 20:00:05.568240  4555 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\n",
      "I0115 20:00:05.568270  4555 net.cpp:253] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0115 20:00:05.568388  4555 net.cpp:42] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    mirror: true\n",
      "    crop_size: 32\n",
      "    mean_file: \"mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"train_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 64\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0115 20:00:05.568487  4555 layer_factory.hpp:74] Creating layer data\n",
      "I0115 20:00:05.568513  4555 net.cpp:76] Creating Layer data\n",
      "I0115 20:00:05.568526  4555 net.cpp:334] data -> data\n",
      "I0115 20:00:05.568554  4555 net.cpp:334] data -> label\n",
      "I0115 20:00:05.568578  4555 net.cpp:105] Setting up data\n",
      "I0115 20:00:05.568675  4555 db.cpp:34] Opened lmdb train_lmdb\n",
      "I0115 20:00:05.568725  4555 data_layer.cpp:67] output data size: 100,3,32,32\n",
      "I0115 20:00:05.568743  4555 data_transformer.cpp:22] Loading mean file from: mean.binaryproto\n",
      "I0115 20:00:05.569705  4555 net.cpp:112] Top shape: 100 3 32 32 (307200)\n",
      "I0115 20:00:05.569742  4555 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 20:00:05.569751  4555 layer_factory.hpp:74] Creating layer conv1\n",
      "I0115 20:00:05.569766  4555 net.cpp:76] Creating Layer conv1\n",
      "I0115 20:00:05.569777  4555 net.cpp:372] conv1 <- data\n",
      "I0115 20:00:05.569792  4555 net.cpp:334] conv1 -> conv1\n",
      "I0115 20:00:05.569808  4555 net.cpp:105] Setting up conv1\n",
      "I0115 20:00:05.629655  4555 net.cpp:112] Top shape: 100 32 32 32 (3276800)\n",
      "I0115 20:00:05.629719  4555 layer_factory.hpp:74] Creating layer pool1\n",
      "I0115 20:00:05.629737  4555 net.cpp:76] Creating Layer pool1\n",
      "I0115 20:00:05.629748  4555 net.cpp:372] pool1 <- conv1\n",
      "I0115 20:00:05.629758  4555 net.cpp:334] pool1 -> pool1\n",
      "I0115 20:00:05.629770  4555 net.cpp:105] Setting up pool1\n",
      "I0115 20:00:05.629920  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.629936  4555 layer_factory.hpp:74] Creating layer relu1\n",
      "I0115 20:00:05.629945  4555 net.cpp:76] Creating Layer relu1\n",
      "I0115 20:00:05.629950  4555 net.cpp:372] relu1 <- pool1\n",
      "I0115 20:00:05.629956  4555 net.cpp:323] relu1 -> pool1 (in-place)\n",
      "I0115 20:00:05.629963  4555 net.cpp:105] Setting up relu1\n",
      "I0115 20:00:05.630009  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.630022  4555 layer_factory.hpp:74] Creating layer norm1\n",
      "I0115 20:00:05.630035  4555 net.cpp:76] Creating Layer norm1\n",
      "I0115 20:00:05.630045  4555 net.cpp:372] norm1 <- pool1\n",
      "I0115 20:00:05.630053  4555 net.cpp:334] norm1 -> norm1\n",
      "I0115 20:00:05.630061  4555 net.cpp:105] Setting up norm1\n",
      "I0115 20:00:05.630096  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.630107  4555 layer_factory.hpp:74] Creating layer conv2\n",
      "I0115 20:00:05.630118  4555 net.cpp:76] Creating Layer conv2\n",
      "I0115 20:00:05.630127  4555 net.cpp:372] conv2 <- norm1\n",
      "I0115 20:00:05.630134  4555 net.cpp:334] conv2 -> conv2\n",
      "I0115 20:00:05.630143  4555 net.cpp:105] Setting up conv2\n",
      "I0115 20:00:05.631193  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.631217  4555 layer_factory.hpp:74] Creating layer relu2\n",
      "I0115 20:00:05.631227  4555 net.cpp:76] Creating Layer relu2\n",
      "I0115 20:00:05.631230  4555 net.cpp:372] relu2 <- conv2\n",
      "I0115 20:00:05.631237  4555 net.cpp:323] relu2 -> conv2 (in-place)\n",
      "I0115 20:00:05.631245  4555 net.cpp:105] Setting up relu2\n",
      "I0115 20:00:05.631290  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.631304  4555 layer_factory.hpp:74] Creating layer pool2\n",
      "I0115 20:00:05.631312  4555 net.cpp:76] Creating Layer pool2\n",
      "I0115 20:00:05.631317  4555 net.cpp:372] pool2 <- conv2\n",
      "I0115 20:00:05.631324  4555 net.cpp:334] pool2 -> pool2\n",
      "I0115 20:00:05.631335  4555 net.cpp:105] Setting up pool2\n",
      "I0115 20:00:05.631453  4555 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 20:00:05.631469  4555 layer_factory.hpp:74] Creating layer norm2\n",
      "I0115 20:00:05.631479  4555 net.cpp:76] Creating Layer norm2\n",
      "I0115 20:00:05.631487  4555 net.cpp:372] norm2 <- pool2\n",
      "I0115 20:00:05.631494  4555 net.cpp:334] norm2 -> norm2\n",
      "I0115 20:00:05.631502  4555 net.cpp:105] Setting up norm2\n",
      "I0115 20:00:05.631515  4555 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 20:00:05.631525  4555 layer_factory.hpp:74] Creating layer conv3\n",
      "I0115 20:00:05.631532  4555 net.cpp:76] Creating Layer conv3\n",
      "I0115 20:00:05.631538  4555 net.cpp:372] conv3 <- norm2\n",
      "I0115 20:00:05.631544  4555 net.cpp:334] conv3 -> conv3\n",
      "I0115 20:00:05.631557  4555 net.cpp:105] Setting up conv3\n",
      "I0115 20:00:05.633388  4555 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 20:00:05.633412  4555 layer_factory.hpp:74] Creating layer relu3\n",
      "I0115 20:00:05.633421  4555 net.cpp:76] Creating Layer relu3\n",
      "I0115 20:00:05.633426  4555 net.cpp:372] relu3 <- conv3\n",
      "I0115 20:00:05.633433  4555 net.cpp:323] relu3 -> conv3 (in-place)\n",
      "I0115 20:00:05.633440  4555 net.cpp:105] Setting up relu3\n",
      "I0115 20:00:05.633482  4555 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 20:00:05.633493  4555 layer_factory.hpp:74] Creating layer pool3\n",
      "I0115 20:00:05.633502  4555 net.cpp:76] Creating Layer pool3\n",
      "I0115 20:00:05.633507  4555 net.cpp:372] pool3 <- conv3\n",
      "I0115 20:00:05.633548  4555 net.cpp:334] pool3 -> pool3\n",
      "I0115 20:00:05.633556  4555 net.cpp:105] Setting up pool3\n",
      "I0115 20:00:05.633605  4555 net.cpp:112] Top shape: 100 64 4 4 (102400)\n",
      "I0115 20:00:05.633615  4555 layer_factory.hpp:74] Creating layer ip1\n",
      "I0115 20:00:05.633627  4555 net.cpp:76] Creating Layer ip1\n",
      "I0115 20:00:05.633636  4555 net.cpp:372] ip1 <- pool3\n",
      "I0115 20:00:05.633643  4555 net.cpp:334] ip1 -> ip1\n",
      "I0115 20:00:05.633654  4555 net.cpp:105] Setting up ip1\n",
      "I0115 20:00:05.633735  4555 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 20:00:05.633749  4555 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 20:00:05.633757  4555 net.cpp:76] Creating Layer loss\n",
      "I0115 20:00:05.633766  4555 net.cpp:372] loss <- ip1\n",
      "I0115 20:00:05.633772  4555 net.cpp:372] loss <- label\n",
      "I0115 20:00:05.633780  4555 net.cpp:334] loss -> loss\n",
      "I0115 20:00:05.633792  4555 net.cpp:105] Setting up loss\n",
      "I0115 20:00:05.633802  4555 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 20:00:05.633864  4555 net.cpp:112] Top shape: 1 1 1 1 (1)\n",
      "I0115 20:00:05.633877  4555 net.cpp:118]     with loss weight 1\n",
      "I0115 20:00:05.633906  4555 net.cpp:163] loss needs backward computation.\n",
      "I0115 20:00:05.633911  4555 net.cpp:163] ip1 needs backward computation.\n",
      "I0115 20:00:05.633915  4555 net.cpp:163] pool3 needs backward computation.\n",
      "I0115 20:00:05.633920  4555 net.cpp:163] relu3 needs backward computation.\n",
      "I0115 20:00:05.633924  4555 net.cpp:163] conv3 needs backward computation.\n",
      "I0115 20:00:05.633929  4555 net.cpp:163] norm2 needs backward computation.\n",
      "I0115 20:00:05.633934  4555 net.cpp:163] pool2 needs backward computation.\n",
      "I0115 20:00:05.633937  4555 net.cpp:163] relu2 needs backward computation.\n",
      "I0115 20:00:05.633942  4555 net.cpp:163] conv2 needs backward computation.\n",
      "I0115 20:00:05.633946  4555 net.cpp:163] norm1 needs backward computation.\n",
      "I0115 20:00:05.633951  4555 net.cpp:163] relu1 needs backward computation.\n",
      "I0115 20:00:05.633955  4555 net.cpp:163] pool1 needs backward computation.\n",
      "I0115 20:00:05.633961  4555 net.cpp:163] conv1 needs backward computation.\n",
      "I0115 20:00:05.633965  4555 net.cpp:165] data does not need backward computation.\n",
      "I0115 20:00:05.633970  4555 net.cpp:201] This network produces output loss\n",
      "I0115 20:00:05.633980  4555 net.cpp:446] Collecting Learning Rate and Weight Decay.\n",
      "I0115 20:00:05.633992  4555 net.cpp:213] Network initialization done.\n",
      "I0115 20:00:05.633996  4555 net.cpp:214] Memory required for data: 36046004\n",
      "I0115 20:00:05.634377  4555 solver.cpp:154] Creating test net (#0) specified by net file: src/train_val.prototxt\n",
      "I0115 20:00:05.634423  4555 net.cpp:253] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0115 20:00:05.634549  4555 net.cpp:42] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    crop_size: 32\n",
      "    mean_file: \"mean.binaryproto\"\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"val_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.0001\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"pool1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 32\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 3\n",
      "    alpha: 5e-05\n",
      "    beta: 0.75\n",
      "    norm_region: WITHIN_CHANNEL\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"conv3\"\n",
      "  convolution_param {\n",
      "    num_output: 64\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool3\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"pool3\"\n",
      "  pooling_param {\n",
      "    pool: AVE\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool3\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 250\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip1\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "I0115 20:00:05.634668  4555 layer_factory.hpp:74] Creating layer data\n",
      "I0115 20:00:05.634695  4555 net.cpp:76] Creating Layer data\n",
      "I0115 20:00:05.634727  4555 net.cpp:334] data -> data\n",
      "I0115 20:00:05.634743  4555 net.cpp:334] data -> label\n",
      "I0115 20:00:05.634757  4555 net.cpp:105] Setting up data\n",
      "I0115 20:00:05.634805  4555 db.cpp:34] Opened lmdb val_lmdb\n",
      "I0115 20:00:05.634832  4555 data_layer.cpp:67] output data size: 100,3,32,32\n",
      "I0115 20:00:05.634846  4555 data_transformer.cpp:22] Loading mean file from: mean.binaryproto\n",
      "I0115 20:00:05.635424  4555 net.cpp:112] Top shape: 100 3 32 32 (307200)\n",
      "I0115 20:00:05.635437  4555 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 20:00:05.635443  4555 layer_factory.hpp:74] Creating layer label_data_1_split\n",
      "I0115 20:00:05.635453  4555 net.cpp:76] Creating Layer label_data_1_split\n",
      "I0115 20:00:05.635463  4555 net.cpp:372] label_data_1_split <- label\n",
      "I0115 20:00:05.635470  4555 net.cpp:334] label_data_1_split -> label_data_1_split_0\n",
      "I0115 20:00:05.635483  4555 net.cpp:334] label_data_1_split -> label_data_1_split_1\n",
      "I0115 20:00:05.635498  4555 net.cpp:105] Setting up label_data_1_split\n",
      "I0115 20:00:05.635506  4555 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 20:00:05.635511  4555 net.cpp:112] Top shape: 100 1 1 1 (100)\n",
      "I0115 20:00:05.635516  4555 layer_factory.hpp:74] Creating layer conv1\n",
      "I0115 20:00:05.635524  4555 net.cpp:76] Creating Layer conv1\n",
      "I0115 20:00:05.635534  4555 net.cpp:372] conv1 <- data\n",
      "I0115 20:00:05.635540  4555 net.cpp:334] conv1 -> conv1\n",
      "I0115 20:00:05.635550  4555 net.cpp:105] Setting up conv1\n",
      "I0115 20:00:05.635856  4555 net.cpp:112] Top shape: 100 32 32 32 (3276800)\n",
      "I0115 20:00:05.635879  4555 layer_factory.hpp:74] Creating layer pool1\n",
      "I0115 20:00:05.635889  4555 net.cpp:76] Creating Layer pool1\n",
      "I0115 20:00:05.635893  4555 net.cpp:372] pool1 <- conv1\n",
      "I0115 20:00:05.635903  4555 net.cpp:334] pool1 -> pool1\n",
      "I0115 20:00:05.635910  4555 net.cpp:105] Setting up pool1\n",
      "I0115 20:00:05.636039  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.636054  4555 layer_factory.hpp:74] Creating layer relu1\n",
      "I0115 20:00:05.636062  4555 net.cpp:76] Creating Layer relu1\n",
      "I0115 20:00:05.636072  4555 net.cpp:372] relu1 <- pool1\n",
      "I0115 20:00:05.636078  4555 net.cpp:323] relu1 -> pool1 (in-place)\n",
      "I0115 20:00:05.636086  4555 net.cpp:105] Setting up relu1\n",
      "I0115 20:00:05.636127  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.636138  4555 layer_factory.hpp:74] Creating layer norm1\n",
      "I0115 20:00:05.636145  4555 net.cpp:76] Creating Layer norm1\n",
      "I0115 20:00:05.636152  4555 net.cpp:372] norm1 <- pool1\n",
      "I0115 20:00:05.636157  4555 net.cpp:334] norm1 -> norm1\n",
      "I0115 20:00:05.636183  4555 net.cpp:105] Setting up norm1\n",
      "I0115 20:00:05.636198  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.636206  4555 layer_factory.hpp:74] Creating layer conv2\n",
      "I0115 20:00:05.636214  4555 net.cpp:76] Creating Layer conv2\n",
      "I0115 20:00:05.636219  4555 net.cpp:372] conv2 <- norm1\n",
      "I0115 20:00:05.636224  4555 net.cpp:334] conv2 -> conv2\n",
      "I0115 20:00:05.636236  4555 net.cpp:105] Setting up conv2\n",
      "I0115 20:00:05.637301  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.637325  4555 layer_factory.hpp:74] Creating layer relu2\n",
      "I0115 20:00:05.637332  4555 net.cpp:76] Creating Layer relu2\n",
      "I0115 20:00:05.637337  4555 net.cpp:372] relu2 <- conv2\n",
      "I0115 20:00:05.637344  4555 net.cpp:323] relu2 -> conv2 (in-place)\n",
      "I0115 20:00:05.637351  4555 net.cpp:105] Setting up relu2\n",
      "I0115 20:00:05.637398  4555 net.cpp:112] Top shape: 100 32 16 16 (819200)\n",
      "I0115 20:00:05.637408  4555 layer_factory.hpp:74] Creating layer pool2\n",
      "I0115 20:00:05.637416  4555 net.cpp:76] Creating Layer pool2\n",
      "I0115 20:00:05.637423  4555 net.cpp:372] pool2 <- conv2\n",
      "I0115 20:00:05.637428  4555 net.cpp:334] pool2 -> pool2\n",
      "I0115 20:00:05.637440  4555 net.cpp:105] Setting up pool2\n",
      "I0115 20:00:05.637486  4555 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 20:00:05.637497  4555 layer_factory.hpp:74] Creating layer norm2\n",
      "I0115 20:00:05.637504  4555 net.cpp:76] Creating Layer norm2\n",
      "I0115 20:00:05.637511  4555 net.cpp:372] norm2 <- pool2\n",
      "I0115 20:00:05.637516  4555 net.cpp:334] norm2 -> norm2\n",
      "I0115 20:00:05.637523  4555 net.cpp:105] Setting up norm2\n",
      "I0115 20:00:05.637537  4555 net.cpp:112] Top shape: 100 32 8 8 (204800)\n",
      "I0115 20:00:05.637547  4555 layer_factory.hpp:74] Creating layer conv3\n",
      "I0115 20:00:05.637554  4555 net.cpp:76] Creating Layer conv3\n",
      "I0115 20:00:05.637560  4555 net.cpp:372] conv3 <- norm2\n",
      "I0115 20:00:05.637567  4555 net.cpp:334] conv3 -> conv3\n",
      "I0115 20:00:05.637574  4555 net.cpp:105] Setting up conv3\n",
      "I0115 20:00:05.639551  4555 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 20:00:05.639575  4555 layer_factory.hpp:74] Creating layer relu3\n",
      "I0115 20:00:05.639581  4555 net.cpp:76] Creating Layer relu3\n",
      "I0115 20:00:05.639587  4555 net.cpp:372] relu3 <- conv3\n",
      "I0115 20:00:05.639596  4555 net.cpp:323] relu3 -> conv3 (in-place)\n",
      "I0115 20:00:05.639602  4555 net.cpp:105] Setting up relu3\n",
      "I0115 20:00:05.639732  4555 net.cpp:112] Top shape: 100 64 8 8 (409600)\n",
      "I0115 20:00:05.639749  4555 layer_factory.hpp:74] Creating layer pool3\n",
      "I0115 20:00:05.639756  4555 net.cpp:76] Creating Layer pool3\n",
      "I0115 20:00:05.639761  4555 net.cpp:372] pool3 <- conv3\n",
      "I0115 20:00:05.639770  4555 net.cpp:334] pool3 -> pool3\n",
      "I0115 20:00:05.639782  4555 net.cpp:105] Setting up pool3\n",
      "I0115 20:00:05.639833  4555 net.cpp:112] Top shape: 100 64 4 4 (102400)\n",
      "I0115 20:00:05.639845  4555 layer_factory.hpp:74] Creating layer ip1\n",
      "I0115 20:00:05.639853  4555 net.cpp:76] Creating Layer ip1\n",
      "I0115 20:00:05.639858  4555 net.cpp:372] ip1 <- pool3\n",
      "I0115 20:00:05.639864  4555 net.cpp:334] ip1 -> ip1\n",
      "I0115 20:00:05.639876  4555 net.cpp:105] Setting up ip1\n",
      "I0115 20:00:05.639955  4555 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 20:00:05.639968  4555 layer_factory.hpp:74] Creating layer ip1_ip1_0_split\n",
      "I0115 20:00:05.639977  4555 net.cpp:76] Creating Layer ip1_ip1_0_split\n",
      "I0115 20:00:05.639986  4555 net.cpp:372] ip1_ip1_0_split <- ip1\n",
      "I0115 20:00:05.639992  4555 net.cpp:334] ip1_ip1_0_split -> ip1_ip1_0_split_0\n",
      "I0115 20:00:05.639999  4555 net.cpp:334] ip1_ip1_0_split -> ip1_ip1_0_split_1\n",
      "I0115 20:00:05.640007  4555 net.cpp:105] Setting up ip1_ip1_0_split\n",
      "I0115 20:00:05.640014  4555 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 20:00:05.640018  4555 net.cpp:112] Top shape: 100 2 1 1 (200)\n",
      "I0115 20:00:05.640023  4555 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 20:00:05.640028  4555 net.cpp:76] Creating Layer loss\n",
      "I0115 20:00:05.640033  4555 net.cpp:372] loss <- ip1_ip1_0_split_0\n",
      "I0115 20:00:05.640038  4555 net.cpp:372] loss <- label_data_1_split_0\n",
      "I0115 20:00:05.640049  4555 net.cpp:334] loss -> loss\n",
      "I0115 20:00:05.640055  4555 net.cpp:105] Setting up loss\n",
      "I0115 20:00:05.640081  4555 layer_factory.hpp:74] Creating layer loss\n",
      "I0115 20:00:05.640138  4555 net.cpp:112] Top shape: 1 1 1 1 (1)\n",
      "I0115 20:00:05.640149  4555 net.cpp:118]     with loss weight 1\n",
      "I0115 20:00:05.640158  4555 layer_factory.hpp:74] Creating layer accuracy\n",
      "I0115 20:00:05.640172  4555 net.cpp:76] Creating Layer accuracy\n",
      "I0115 20:00:05.640182  4555 net.cpp:372] accuracy <- ip1_ip1_0_split_1\n",
      "I0115 20:00:05.640188  4555 net.cpp:372] accuracy <- label_data_1_split_1\n",
      "I0115 20:00:05.640193  4555 net.cpp:334] accuracy -> accuracy\n",
      "I0115 20:00:05.640207  4555 net.cpp:105] Setting up accuracy\n",
      "I0115 20:00:05.640215  4555 net.cpp:112] Top shape: 1 1 1 1 (1)\n",
      "I0115 20:00:05.640224  4555 net.cpp:165] accuracy does not need backward computation.\n",
      "I0115 20:00:05.640228  4555 net.cpp:163] loss needs backward computation.\n",
      "I0115 20:00:05.640233  4555 net.cpp:163] ip1_ip1_0_split needs backward computation.\n",
      "I0115 20:00:05.640238  4555 net.cpp:163] ip1 needs backward computation.\n",
      "I0115 20:00:05.640241  4555 net.cpp:163] pool3 needs backward computation.\n",
      "I0115 20:00:05.640245  4555 net.cpp:163] relu3 needs backward computation.\n",
      "I0115 20:00:05.640250  4555 net.cpp:163] conv3 needs backward computation.\n",
      "I0115 20:00:05.640254  4555 net.cpp:163] norm2 needs backward computation.\n",
      "I0115 20:00:05.640259  4555 net.cpp:163] pool2 needs backward computation.\n",
      "I0115 20:00:05.640264  4555 net.cpp:163] relu2 needs backward computation.\n",
      "I0115 20:00:05.640267  4555 net.cpp:163] conv2 needs backward computation.\n",
      "I0115 20:00:05.640272  4555 net.cpp:163] norm1 needs backward computation.\n",
      "I0115 20:00:05.640276  4555 net.cpp:163] relu1 needs backward computation.\n",
      "I0115 20:00:05.640282  4555 net.cpp:163] pool1 needs backward computation.\n",
      "I0115 20:00:05.640285  4555 net.cpp:163] conv1 needs backward computation.\n",
      "I0115 20:00:05.640290  4555 net.cpp:165] label_data_1_split does not need backward computation.\n",
      "I0115 20:00:05.640295  4555 net.cpp:165] data does not need backward computation.\n",
      "I0115 20:00:05.640298  4555 net.cpp:201] This network produces output accuracy\n",
      "I0115 20:00:05.640305  4555 net.cpp:201] This network produces output loss\n",
      "I0115 20:00:05.640316  4555 net.cpp:446] Collecting Learning Rate and Weight Decay.\n",
      "I0115 20:00:05.640326  4555 net.cpp:213] Network initialization done.\n",
      "I0115 20:00:05.640329  4555 net.cpp:214] Memory required for data: 36048408\n",
      "I0115 20:00:05.640372  4555 solver.cpp:42] Solver scaffolding done.\n",
      "I0115 20:00:05.640396  4555 solver.cpp:222] Solving \n",
      "I0115 20:00:05.640405  4555 solver.cpp:223] Learning Rate Policy: step\n",
      "I0115 20:00:05.640415  4555 solver.cpp:266] Iteration 0, Testing net (#0)\n",
      "I0115 20:00:06.453776  4555 solver.cpp:315]     Test net output #0: accuracy = 0.5044\n",
      "I0115 20:00:06.453820  4555 solver.cpp:315]     Test net output #1: loss = 0.693134 (* 1 = 0.693134 loss)\n",
      "I0115 20:00:06.465080  4555 solver.cpp:189] Iteration 0, loss = 0.693167\n",
      "I0115 20:00:06.465109  4555 solver.cpp:204]     Train net output #0: loss = 0.693167 (* 1 = 0.693167 loss)\n",
      "I0115 20:00:06.465128  4555 solver.cpp:470] Iteration 0, lr = 0.005\n",
      "I0115 20:00:06.915690  4555 solver.cpp:189] Iteration 20, loss = 0.691088\n",
      "I0115 20:00:06.915730  4555 solver.cpp:204]     Train net output #0: loss = 0.691088 (* 1 = 0.691088 loss)\n",
      "I0115 20:00:06.915738  4555 solver.cpp:470] Iteration 20, lr = 0.005\n",
      "I0115 20:00:07.367889  4555 solver.cpp:189] Iteration 40, loss = 0.696413\n",
      "I0115 20:00:07.367928  4555 solver.cpp:204]     Train net output #0: loss = 0.696413 (* 1 = 0.696413 loss)\n",
      "I0115 20:00:07.367936  4555 solver.cpp:470] Iteration 40, lr = 0.005\n",
      "I0115 20:00:07.819696  4555 solver.cpp:189] Iteration 60, loss = 0.668123\n",
      "I0115 20:00:07.819730  4555 solver.cpp:204]     Train net output #0: loss = 0.668123 (* 1 = 0.668123 loss)\n",
      "I0115 20:00:07.819738  4555 solver.cpp:470] Iteration 60, lr = 0.005\n",
      "I0115 20:00:08.271719  4555 solver.cpp:189] Iteration 80, loss = 0.652772\n",
      "I0115 20:00:08.271754  4555 solver.cpp:204]     Train net output #0: loss = 0.652772 (* 1 = 0.652772 loss)\n",
      "I0115 20:00:08.271764  4555 solver.cpp:470] Iteration 80, lr = 0.005\n",
      "I0115 20:00:08.723562  4555 solver.cpp:189] Iteration 100, loss = 0.672793\n",
      "I0115 20:00:08.723599  4555 solver.cpp:204]     Train net output #0: loss = 0.672793 (* 1 = 0.672793 loss)\n",
      "I0115 20:00:08.723614  4555 solver.cpp:470] Iteration 100, lr = 0.005\n",
      "I0115 20:00:09.175863  4555 solver.cpp:189] Iteration 120, loss = 0.687575\n",
      "I0115 20:00:09.175901  4555 solver.cpp:204]     Train net output #0: loss = 0.687575 (* 1 = 0.687575 loss)\n",
      "I0115 20:00:09.175910  4555 solver.cpp:470] Iteration 120, lr = 0.005\n",
      "I0115 20:00:09.627962  4555 solver.cpp:189] Iteration 140, loss = 0.634447\n",
      "I0115 20:00:09.628016  4555 solver.cpp:204]     Train net output #0: loss = 0.634447 (* 1 = 0.634447 loss)\n",
      "I0115 20:00:09.628026  4555 solver.cpp:470] Iteration 140, lr = 0.005\n",
      "I0115 20:00:10.079421  4555 solver.cpp:189] Iteration 160, loss = 0.620599\n",
      "I0115 20:00:10.079473  4555 solver.cpp:204]     Train net output #0: loss = 0.620599 (* 1 = 0.620599 loss)\n",
      "I0115 20:00:10.079483  4555 solver.cpp:470] Iteration 160, lr = 0.005\n",
      "I0115 20:00:10.530860  4555 solver.cpp:189] Iteration 180, loss = 0.655782\n",
      "I0115 20:00:10.530906  4555 solver.cpp:204]     Train net output #0: loss = 0.655782 (* 1 = 0.655782 loss)\n",
      "I0115 20:00:10.530915  4555 solver.cpp:470] Iteration 180, lr = 0.005\n",
      "I0115 20:00:10.983217  4555 solver.cpp:189] Iteration 200, loss = 0.662885\n",
      "I0115 20:00:10.983256  4555 solver.cpp:204]     Train net output #0: loss = 0.662885 (* 1 = 0.662885 loss)\n",
      "I0115 20:00:10.983264  4555 solver.cpp:470] Iteration 200, lr = 0.005\n",
      "I0115 20:00:11.435542  4555 solver.cpp:189] Iteration 220, loss = 0.619748\n",
      "I0115 20:00:11.435580  4555 solver.cpp:204]     Train net output #0: loss = 0.619748 (* 1 = 0.619748 loss)\n",
      "I0115 20:00:11.435588  4555 solver.cpp:470] Iteration 220, lr = 0.005\n",
      "I0115 20:00:11.888103  4555 solver.cpp:189] Iteration 240, loss = 0.623545\n",
      "I0115 20:00:11.888135  4555 solver.cpp:204]     Train net output #0: loss = 0.623545 (* 1 = 0.623545 loss)\n",
      "I0115 20:00:11.888144  4555 solver.cpp:470] Iteration 240, lr = 0.005\n",
      "I0115 20:00:12.106103  4555 solver.cpp:334] Snapshotting to checkpoints/snapshot_iter_250.caffemodel\n",
      "I0115 20:00:12.107713  4555 solver.cpp:342] Snapshotting solver state to checkpoints/snapshot_iter_250.solverstate\n",
      "I0115 20:00:12.108485  4555 solver.cpp:266] Iteration 250, Testing net (#0)\n",
      "I0115 20:00:12.914660  4555 solver.cpp:315]     Test net output #0: accuracy = 0.6052\n",
      "I0115 20:00:12.914697  4555 solver.cpp:315]     Test net output #1: loss = 0.648628 (* 1 = 0.648628 loss)\n",
      "I0115 20:00:13.149204  4555 solver.cpp:189] Iteration 260, loss = 0.665646\n",
      "I0115 20:00:13.149236  4555 solver.cpp:204]     Train net output #0: loss = 0.665646 (* 1 = 0.665646 loss)\n",
      "I0115 20:00:13.149245  4555 solver.cpp:470] Iteration 260, lr = 0.005\n",
      "I0115 20:00:13.601188  4555 solver.cpp:189] Iteration 280, loss = 0.646074\n",
      "I0115 20:00:13.601223  4555 solver.cpp:204]     Train net output #0: loss = 0.646074 (* 1 = 0.646074 loss)\n",
      "I0115 20:00:13.601233  4555 solver.cpp:470] Iteration 280, lr = 0.005\n",
      "I0115 20:00:14.053146  4555 solver.cpp:189] Iteration 300, loss = 0.636224\n",
      "I0115 20:00:14.053177  4555 solver.cpp:204]     Train net output #0: loss = 0.636224 (* 1 = 0.636224 loss)\n",
      "I0115 20:00:14.053186  4555 solver.cpp:470] Iteration 300, lr = 0.005\n",
      "I0115 20:00:14.506160  4555 solver.cpp:189] Iteration 320, loss = 0.649575\n",
      "I0115 20:00:14.506206  4555 solver.cpp:204]     Train net output #0: loss = 0.649575 (* 1 = 0.649575 loss)\n",
      "I0115 20:00:14.506214  4555 solver.cpp:470] Iteration 320, lr = 0.005\n",
      "I0115 20:00:14.958771  4555 solver.cpp:189] Iteration 340, loss = 0.572727\n",
      "I0115 20:00:14.958813  4555 solver.cpp:204]     Train net output #0: loss = 0.572727 (* 1 = 0.572727 loss)\n",
      "I0115 20:00:14.958822  4555 solver.cpp:470] Iteration 340, lr = 0.005\n",
      "I0115 20:00:15.411084  4555 solver.cpp:189] Iteration 360, loss = 0.63059\n",
      "I0115 20:00:15.411119  4555 solver.cpp:204]     Train net output #0: loss = 0.63059 (* 1 = 0.63059 loss)\n",
      "I0115 20:00:15.411128  4555 solver.cpp:470] Iteration 360, lr = 0.005\n",
      "I0115 20:00:15.863382  4555 solver.cpp:189] Iteration 380, loss = 0.62042\n",
      "I0115 20:00:15.863453  4555 solver.cpp:204]     Train net output #0: loss = 0.62042 (* 1 = 0.62042 loss)\n",
      "I0115 20:00:15.863461  4555 solver.cpp:470] Iteration 380, lr = 0.005\n",
      "I0115 20:00:16.316114  4555 solver.cpp:189] Iteration 400, loss = 0.561114\n",
      "I0115 20:00:16.316149  4555 solver.cpp:204]     Train net output #0: loss = 0.561114 (* 1 = 0.561114 loss)\n",
      "I0115 20:00:16.316157  4555 solver.cpp:470] Iteration 400, lr = 0.005\n",
      "I0115 20:00:16.768280  4555 solver.cpp:189] Iteration 420, loss = 0.608278\n",
      "I0115 20:00:16.768319  4555 solver.cpp:204]     Train net output #0: loss = 0.608278 (* 1 = 0.608278 loss)\n",
      "I0115 20:00:16.768327  4555 solver.cpp:470] Iteration 420, lr = 0.005\n",
      "I0115 20:00:17.220721  4555 solver.cpp:189] Iteration 440, loss = 0.630416\n",
      "I0115 20:00:17.220773  4555 solver.cpp:204]     Train net output #0: loss = 0.630416 (* 1 = 0.630416 loss)\n",
      "I0115 20:00:17.220782  4555 solver.cpp:470] Iteration 440, lr = 0.005\n",
      "I0115 20:00:17.673382  4555 solver.cpp:189] Iteration 460, loss = 0.584583\n",
      "I0115 20:00:17.673423  4555 solver.cpp:204]     Train net output #0: loss = 0.584583 (* 1 = 0.584583 loss)\n",
      "I0115 20:00:17.673430  4555 solver.cpp:470] Iteration 460, lr = 0.005\n",
      "I0115 20:00:18.125972  4555 solver.cpp:189] Iteration 480, loss = 0.598507\n",
      "I0115 20:00:18.126008  4555 solver.cpp:204]     Train net output #0: loss = 0.598507 (* 1 = 0.598507 loss)\n",
      "I0115 20:00:18.126016  4555 solver.cpp:470] Iteration 480, lr = 0.005\n",
      "I0115 20:00:18.570288  4555 solver.cpp:334] Snapshotting to checkpoints/snapshot_iter_500.caffemodel\n",
      "I0115 20:00:18.571691  4555 solver.cpp:342] Snapshotting solver state to checkpoints/snapshot_iter_500.solverstate\n",
      "I0115 20:00:18.572458  4555 solver.cpp:266] Iteration 500, Testing net (#0)\n",
      "I0115 20:00:19.380465  4555 solver.cpp:315]     Test net output #0: accuracy = 0.6939\n",
      "I0115 20:00:19.380511  4555 solver.cpp:315]     Test net output #1: loss = 0.584462 (* 1 = 0.584462 loss)\n",
      "I0115 20:00:19.389122  4555 solver.cpp:189] Iteration 500, loss = 0.594391\n",
      "I0115 20:00:19.389150  4555 solver.cpp:204]     Train net output #0: loss = 0.594391 (* 1 = 0.594391 loss)\n",
      "I0115 20:00:19.389160  4555 solver.cpp:470] Iteration 500, lr = 0.0005\n",
      "I0115 20:00:19.841200  4555 solver.cpp:189] Iteration 520, loss = 0.597431\n",
      "I0115 20:00:19.841253  4555 solver.cpp:204]     Train net output #0: loss = 0.597431 (* 1 = 0.597431 loss)\n",
      "I0115 20:00:19.841262  4555 solver.cpp:470] Iteration 520, lr = 0.0005\n",
      "I0115 20:00:20.293457  4555 solver.cpp:189] Iteration 540, loss = 0.535105\n",
      "I0115 20:00:20.293498  4555 solver.cpp:204]     Train net output #0: loss = 0.535105 (* 1 = 0.535105 loss)\n",
      "I0115 20:00:20.293505  4555 solver.cpp:470] Iteration 540, lr = 0.0005\n",
      "I0115 20:00:20.745051  4555 solver.cpp:189] Iteration 560, loss = 0.557655\n",
      "I0115 20:00:20.745100  4555 solver.cpp:204]     Train net output #0: loss = 0.557655 (* 1 = 0.557655 loss)\n",
      "I0115 20:00:20.745111  4555 solver.cpp:470] Iteration 560, lr = 0.0005\n",
      "I0115 20:00:21.197535  4555 solver.cpp:189] Iteration 580, loss = 0.544688\n",
      "I0115 20:00:21.197573  4555 solver.cpp:204]     Train net output #0: loss = 0.544688 (* 1 = 0.544688 loss)\n",
      "I0115 20:00:21.197582  4555 solver.cpp:470] Iteration 580, lr = 0.0005\n",
      "I0115 20:00:21.649377  4555 solver.cpp:189] Iteration 600, loss = 0.466709\n",
      "I0115 20:00:21.649420  4555 solver.cpp:204]     Train net output #0: loss = 0.466709 (* 1 = 0.466709 loss)\n",
      "I0115 20:00:21.649430  4555 solver.cpp:470] Iteration 600, lr = 0.0005\n",
      "I0115 20:00:22.101996  4555 solver.cpp:189] Iteration 620, loss = 0.529726\n",
      "I0115 20:00:22.102030  4555 solver.cpp:204]     Train net output #0: loss = 0.529726 (* 1 = 0.529726 loss)\n",
      "I0115 20:00:22.102037  4555 solver.cpp:470] Iteration 620, lr = 0.0005\n",
      "I0115 20:00:22.554704  4555 solver.cpp:189] Iteration 640, loss = 0.535052\n",
      "I0115 20:00:22.554760  4555 solver.cpp:204]     Train net output #0: loss = 0.535052 (* 1 = 0.535052 loss)\n",
      "I0115 20:00:22.554769  4555 solver.cpp:470] Iteration 640, lr = 0.0005\n",
      "I0115 20:00:23.006484  4555 solver.cpp:189] Iteration 660, loss = 0.515144\n",
      "I0115 20:00:23.006516  4555 solver.cpp:204]     Train net output #0: loss = 0.515144 (* 1 = 0.515144 loss)\n",
      "I0115 20:00:23.006561  4555 solver.cpp:470] Iteration 660, lr = 0.0005\n",
      "I0115 20:00:23.458658  4555 solver.cpp:189] Iteration 680, loss = 0.613662\n",
      "I0115 20:00:23.458690  4555 solver.cpp:204]     Train net output #0: loss = 0.613662 (* 1 = 0.613662 loss)\n",
      "I0115 20:00:23.458699  4555 solver.cpp:470] Iteration 680, lr = 0.0005\n",
      "I0115 20:00:23.910390  4555 solver.cpp:189] Iteration 700, loss = 0.582863\n",
      "I0115 20:00:23.910425  4555 solver.cpp:204]     Train net output #0: loss = 0.582863 (* 1 = 0.582863 loss)\n",
      "I0115 20:00:23.910434  4555 solver.cpp:470] Iteration 700, lr = 0.0005\n",
      "I0115 20:00:24.362695  4555 solver.cpp:189] Iteration 720, loss = 0.565415\n",
      "I0115 20:00:24.362756  4555 solver.cpp:204]     Train net output #0: loss = 0.565415 (* 1 = 0.565415 loss)\n",
      "I0115 20:00:24.362764  4555 solver.cpp:470] Iteration 720, lr = 0.0005\n",
      "I0115 20:00:24.814307  4555 solver.cpp:189] Iteration 740, loss = 0.482684\n",
      "I0115 20:00:24.814347  4555 solver.cpp:204]     Train net output #0: loss = 0.482684 (* 1 = 0.482684 loss)\n",
      "I0115 20:00:24.814355  4555 solver.cpp:470] Iteration 740, lr = 0.0005\n",
      "I0115 20:00:25.032222  4555 solver.cpp:334] Snapshotting to checkpoints/snapshot_iter_750.caffemodel\n",
      "I0115 20:00:25.033501  4555 solver.cpp:342] Snapshotting solver state to checkpoints/snapshot_iter_750.solverstate\n",
      "I0115 20:00:25.034466  4555 solver.cpp:266] Iteration 750, Testing net (#0)\n",
      "I0115 20:00:25.840834  4555 solver.cpp:315]     Test net output #0: accuracy = 0.7124\n",
      "I0115 20:00:25.840874  4555 solver.cpp:315]     Test net output #1: loss = 0.562013 (* 1 = 0.562013 loss)\n",
      "I0115 20:00:25.840883  4555 solver.cpp:253] Optimization Done.\n",
      "I0115 20:00:25.840886  4555 caffe.cpp:121] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "TOOLS=/home/ubuntu/caffe/build/tools\n",
    "#Train your modified network configuration\n",
    "$TOOLS/caffe train -gpu 0 -solver src/solver2.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now trained a network architecture with a much larger number of trainable parameters.\n",
    "\n",
    "####Q #2:\n",
    "Did you notice any performance improvements?  If so, what is your accuracy now?\n",
    "\n",
    "**A**: See [Answer #2 below](#Answer-#2)\n",
    "\n",
    "Good work, we are underfitting less.  Let's make one more modification to the network before training a final time.  In the last modification we increased the number of neurons in our convolutional layers.  Another way to increase the number of trainable parameters in our network is to make it deeper by adding more layers.  This time you should add two new fully-connected layers with 100 outputs each - call them `ip2` and `ip3`.  These layers should come after the third pooling layer, `pool3`, but **before** the existing fully-connected layer `ip1`.  In Caffe, fully-connected layers are implemented using the inner product layer construct.  After the new fully-connected (inner product) layer you also need a ReLU activation layer and a dropout layer.  The dropout layer will prevent the network from *overfitting*, i.e. getting really good at classifying the training data but not able to classify the validation data.\n",
    "\n",
    "Here is what `ip2` should look like when inserted after `pool3`.  You'll need to modify the layer names to create `ip3` after `ip2`.\n",
    "\n",
    "REMEMBER: Don't forget to change the input of your existing fully-connected layer `ip1` to be the output of the new layer `ip3`!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layer {\n",
    "  name: \"ip2\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"pool3\"\n",
    "  top: \"ip2\"\n",
    "  inner_product_param {\n",
    "\tnum_output: 100\n",
    "\tweight_filler {\n",
    "  \ttype: \"gaussian\"\n",
    "  \tstd: 0.1\n",
    "\t}\n",
    "\tbias_filler {\n",
    "  \ttype: \"constant\"\n",
    "  \tvalue:0.9\n",
    "\t}\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"reluip2\"\n",
    "  type: \"ReLU\"\n",
    "  bottom: \"ip2\"\n",
    "  top: \"ip2\"\n",
    "}\n",
    "layer {\n",
    "  name: \"dropip2\"\n",
    "  type: \"Dropout\"\n",
    "  bottom: \"ip2\"\n",
    "  top: \"ip2\"\n",
    "  dropout_param {\n",
    "    dropout_ratio: 0.5\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `train_val3.prototxt` file below by adding the new layers and then activate the subsequent cell to train one more time. Again, this model will take slightly longer to train due to the increased size.\n",
    "\n",
    "Again, if you have any problems modifying the file, you can look at the answer in `train_val3.answer.prototxt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe id=\"task3\" src=\"/task3\" width=\"100%\" height=\"500px\">\n",
    " <p>Your browser does not support iframes.</p>\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "TOOLS=/home/ubuntu/caffe/build/tools\n",
    "#Train your modified network configuration\n",
    "$TOOLS/caffe train -gpu 0 -solver src/solver3.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Q #3:\n",
    "What is the classification accuracy now?\n",
    "\n",
    "**A**: See [Answer #3 below](#Answer-#3)\n",
    "\n",
    "We have now seen two ways in which a model can be modified in Caffe to improve classication accuracy - adding more layers or increasing the number of neurons in existing layers.  There are many more ways to improve classification accuracy for you to explore and we will cover some of them in subsequent classes.\n",
    "\n",
    "####Q #4:\n",
    "What else might you do to improve classification accuracy?\n",
    "\n",
    "**A**: See [Answer #4 below](#Answer-#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 4  - Classification\n",
    "\n",
    "We will now learn how to deploy our final trained network to perform classification of new images. For all of the training above we used the Caffe command line interface tools.  For classification we are going to use Caffe's Python interface.  We will first import the Python libraries we require and create some variables specifying the locations of important files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6.0, 4.0)\n",
    "\n",
    "# Make sure that caffe is on the python path:\n",
    "#caffe_root = '../'  # this file is expected to be in {caffe_root}/examples\n",
    "import sys\n",
    "#sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "\n",
    "# Set the right path to your model definition file, pretrained model weights,\n",
    "# and the image you would like to classify.\n",
    "MODEL_FILE = '/home/ubuntu/notebook/src/deploy3.prototxt'\n",
    "PRETRAINED = '/home/ubuntu/notebook/checkpoints/pretrained.caffemodel'\n",
    "IMAGE_FILE1 = '/home/ubuntu/data/dog_cat/dog_cat_32/test/cat_236.jpg'\n",
    "IMAGE_FILE2 = '/home/ubuntu/data/dog_cat/dog_cat_32/test/dog_4987.jpg'\n",
    "LABELS_FILE = '/home/ubuntu/data/dog_cat/dog_cat_32/labels.txt'\n",
    "labels=open(LABELS_FILE,'r').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a network is easy. The `caffe.Classifier` method takes care of everything. Note the arguments for configuring input preprocessing: mean subtraction switched on by giving a mean array, input channel swapping takes care of mapping RGB into the reference ImageNet model's BGR order, and raw scaling multiplies the feature scale from the input [0,1] to the ImageNet model's [0,255]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we must import the mean.binaryproto mean image into a numpy array\n",
    "blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "data = open( 'mean.binaryproto' , 'rb' ).read()\n",
    "blob.ParseFromString(data)\n",
    "arr = np.array( caffe.io.blobproto_to_array(blob) )\n",
    "out = arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load our pretrained model\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "                       mean=out,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(32, 32))\n",
    "net2 = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
    "                       mean=out,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our example images with Caffe's image loading helper. We are going to classify 2 different images, one from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load two test images\n",
    "input_image1 = caffe.io.load_image(IMAGE_FILE1)\n",
    "input_image2 = caffe.io.load_image(IMAGE_FILE2)\n",
    "# Display the test images\n",
    "plt.subplot(1,4,1).imshow(input_image1),plt.title('Cat')\n",
    "plt.subplot(1,4,2).imshow(input_image2),plt.title('Dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to classify. The default is to actually do 10 predictions, cropping the center and corners of the image as well as their mirrored versions, and average over the predictions.  This approach typically leads to better classification accuracy as it is more robust to object translation within in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction1 = net.predict([input_image1]) \n",
    "prediction2 = net2.predict([input_image2])\n",
    "width=0.1\n",
    "plt.bar(np.arange(2),prediction1[0],width,color='blue',label='Cat')\n",
    "plt.bar(np.arange(2)+width,prediction2[0],width,color='green',label='Dog')\n",
    "plt.xticks(np.arange(2)+width,labels)\n",
    "plt.ylabel('Class Probability')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see what class the neural network believes each image is. In the cases above the highest probabilities are given to the correct class for both test images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Filter Visualization \n",
    "\n",
    "This portion of the lesson follows the filter visualization example provided with Caffe and the DeCAF visualizations originally developed by Yangqing Jia.\n",
    "\n",
    "In this task you are going to visualize the network's response from the two images classified in Task 2. In a convolutional layer in a deep neural network the weights that connect the layer inputs to the outputs form a four-dimensional tensor.  You can think of this tensor as being a collection of small two-dimensional arrays with multiple channels (you could also think of each of these as a three-dimensional array).  It is these arrays which are convolved with the input to the layer to produce the layers output activations.  In our final network above the first convolutional layer had 64 three-channel 5x5 weights.  These small arrays are often referred to as (convolutional) *filters*.  When we convolve these filters with the layer input we obtain what are often referred to as *feature maps*.  In this task we will treat these network activations as images as this is often useful for understanding what the network has actually learned during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#View a list of the network layer outputs and their dimensions\n",
    "[(k, v.data.shape) for k, v in net.blobs.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you are going to visualize the filters of the first layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take an array of shape (n, height, width) or (n, height, width, channels)\n",
    "# and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\n",
    "def vis_square(data, padsize=1, padval=0):\n",
    "    data -= data.min()\n",
    "    data /= data.max()\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    plt.imshow(data)\n",
    "\n",
    "    # the parameters are a list of [weights, biases]\n",
    "\n",
    "###########################################################################################################################\n",
    "#TODO: All of the weights of the first layer are plotted below. Modify the filters parameter so that you can view some of \n",
    "#the weights more closely. Try looking at the first 10 and 20 filters.  \n",
    "##########################################################################################################################\n",
    "plt.rcParams['figure.figsize'] = (25.0, 20.0)\n",
    "filters = net.params['conv1'][0].data\n",
    "vis_square(filters.transpose(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are going to view the feature maps of the two input images after they have been processed by the first convolutional layer. Feel free to modify the feat variables so that you can take a closer look as some of the feature maps more closely. Notice the visual similarities and differences between the features maps of both of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['conv1'].data[0,:64]\n",
    "plt.subplot(1,2,1),plt.title('Cat')\n",
    "vis_square(feat, padval=1)\n",
    "net.blobs['conv1'].data.shape\n",
    "feat2 = net2.blobs['conv1'].data[0, :64]\n",
    "plt.subplot(1,2,2),plt.title('Dog')\n",
    "vis_square(feat2, padval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see many differences between the networks responses for the two different input images?\n",
    "\n",
    "Now view the feature maps from the 2nd convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['conv2'].data[0]\n",
    "plt.subplot(1,2,1),plt.title('Cat')\n",
    "vis_square(feat, padval=1)\n",
    "feat2 = net2.blobs['conv2'].data[0]\n",
    "plt.subplot(1,2,2),plt.title('Dog')\n",
    "vis_square(feat2, padval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now view the feature map of the last convolutional layer and then the pooled version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['conv3'].data[0]\n",
    "plt.subplot(1,2,1),plt.title('Cat')\n",
    "vis_square(feat, padval=0.5)\n",
    "feat2 = net2.blobs['conv3'].data[0]\n",
    "plt.subplot(1,2,2),plt.title('Dog')\n",
    "vis_square(feat2, padval=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['pool3'].data[0,:100]\n",
    "plt.subplot(1,2,1),plt.title('Cat')\n",
    "vis_square(feat, padval=1)\n",
    "feat2 = net2.blobs['pool3'].data[0,:100]\n",
    "plt.subplot(1,2,2),plt.title('Dog')\n",
    "vis_square(feat2, padval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now view the neuron activations for the fully-connected layer ip2. You will notice that the neurons being activated by the two input images are very different. This is good as it means the network is effectively differentiating the two images at the higher layers in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat = net.blobs['ip2'].data[0]\n",
    "plt.plot(feat.flat,label='Cat')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "##########################################################################################################################\n",
    "# Plot ip2 for the input image of the Dog image. Compare the Differences \n",
    "feat2 = net2.blobs['ip2'].data[0]\n",
    "plt.plot(feat2.flat, label='Dog')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 6 - Classifying Many Images\n",
    "\n",
    "A text file containing a list of 20 images being stored on this host machine is provided. By executing the cell below you will classify all of these images with the network you trained above and calculate the mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEST_FILE=open('/home/ubuntu/data/dog_cat/dog_cat_32/val/test.txt','r') \n",
    "TEST_IMAGES=TEST_FILE.readlines()\n",
    "PredictScore=np.zeros((len(TEST_IMAGES),1))\n",
    "for i in range(len(TEST_IMAGES)):\n",
    "    IMAGE_FILE='/home/ubuntu/data/dog_cat/dog_cat_32/val/' + TEST_IMAGES[i].split()[0]\n",
    "    CATEGORY=TEST_IMAGES[i].split()[1]\n",
    "    #print TEST_IMAGES[i]\n",
    "    input_test = caffe.io.load_image(IMAGE_FILE)\n",
    "    prediction = net2.predict([input_test])  \n",
    "    #print prediction[0]\n",
    "    if prediction[0].argmax()==int(CATEGORY):\n",
    "        print 'CORRECT -- predicted class for ', str(IMAGE_FILE[62:]),':', prediction[0].argmax(), 'true class:', CATEGORY\n",
    "    elif prediction[0].argmax()!=int(CATEGORY):\n",
    "        print 'WRONG -- predicted class ', str(IMAGE_FILE[62:]),':', prediction[0].argmax(), 'true class:', CATEGORY            \n",
    "    PredictScore[i]=int(prediction[0].argmax()==int(CATEGORY))\n",
    "Accuracy=np.sum(PredictScore)/len(PredictScore)\n",
    "print 'Prediction accuracy with this image set is', np.sum(PredictScore)/len(PredictScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommend you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "## More information\n",
    "\n",
    "For more information on using Caffe, visit http://caffe.berkeleyvision.org/. A description of the framework, how to use it, and plenty of examples similar to this lesson are posted. \n",
    "\n",
    "To learn more about these other topics, please visit:\n",
    "* GPU accelerated machine learning: [http://www.nvidia.com/object/machine-learning.html](http://www.nvidia.com/object/machine-learning.html)\n",
    "* Theano: [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)\n",
    "* Torch: [http://torch.ch/](http://torch.ch/)\n",
    "* DIGITS: [https://developer.nvidia.com/digits](https://developer.nvidia.com/digits)\n",
    "* cuDNN: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)\n",
    "\n",
    "### Deep Learning Lab Series\n",
    "\n",
    "Make sure to check out the rest of the classes in this Deep Learning lab series.  You can find them [here](https://developer.nvidia.com/deep-learning-courses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Answers\n",
    "\n",
    "### Answer #1\n",
    "About 60% after 250, 65% after 500 and 69% after 750\n",
    "\n",
    "[Return to question](#Q-#1:)\n",
    "\n",
    "### Answer #2\n",
    "You should see an improvement to over 70%\n",
    "\n",
    "[Return to question](#Q-#2:)\n",
    "\n",
    "### Answer #3\n",
    "Over 73%\n",
    "\n",
    "[Return to question](#Q-#3:)\n",
    "\n",
    "### Answer #4\n",
    "Increase network size, change activation functions, modify training algorithm, data augmentation\n",
    "\n",
    "[Return to question](#Q-#4:)"
   ]
  }
 ],
 "metadata": {
  "description": "Use the pre-trained ImageNet model to classify images with the Python interface.",
  "example_name": "ImageNet classification",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 1
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
